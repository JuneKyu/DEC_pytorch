{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.cluster import KMeans\n",
    "## load mnist dataset\n",
    "use_cuda = torch.cuda.is_available()\n",
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "#trans = transforms.Compose([transforms.Resize(32), transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "# if not exist, download mnist dataset\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)\n",
    "batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timingResult = {}\n",
    "def logTime(theName, currentTime):\n",
    "    if theName not in timingResult:\n",
    "        timingResult[theName] = time.time() - currentTime\n",
    "    else:\n",
    "        timingResult[theName] = timingResult[theName] + (time.time() - currentTime)\n",
    "    currentTime = time.time()\n",
    "    return currentTime\n",
    "\n",
    "def printTiming(name):\n",
    "    print('======== timing for {}: {} ======='.format(name,timingResult[name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEC_AE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DEC_AE,self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.conv_ae1 = nn.Conv2d(1,50,4,stride=2,padding=2)\n",
    "        self.conv_ae2 = nn.Conv2d(50,50,5,stride=2,padding=2)\n",
    "        self.leReLU = nn.LeakyReLU()\n",
    "        self.fc1 = nn.Linear(50*8*8,48)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc_de = nn.Linear(48,50*8*8)\n",
    "        self.conv_de2 = nn.ConvTranspose2d(50,50,5,stride=2,padding=2)\n",
    "        self.conv_de1 = nn.ConvTranspose2d(50,1,4,stride=2,padding=2)\n",
    "        self.pretrainMode = True\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform(m.weight)\n",
    "\n",
    "    def pretrainMode(self,mode):\n",
    "        \"\"\"To set training mode to pretrain or not, \n",
    "        so that it can control to run only the AE or AE+DECODER\"\"\"\n",
    "        self.pretrainMode = mode\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # 32x32x1\n",
    "        x = self.dropout(x)\n",
    "        # 32x32x1\n",
    "        x = self.conv_ae1(x)\n",
    "        # 17x17x50\n",
    "        x = self.leReLU(x)\n",
    "        # 17x17x50\n",
    "        x = self.dropout(x)\n",
    "        # 17x17x50\n",
    "        x = self.conv_ae2(x)\n",
    "        # 9x9x50\n",
    "        x = self.leReLU(x)\n",
    "        # 9x9x50\n",
    "        x = self.dropout(x)\n",
    "        # 9x9x50\n",
    "        x = x.view(-1, 50*8*8)\n",
    "        # 1x4050\n",
    "        x = self.fc1(x)\n",
    "        # 1x68\n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        x_ae = x # this is the returned auto encoder\n",
    "        \n",
    "        #if not in pretrain mode, we only need encoder\n",
    "        if self.pretrainMode == False:\n",
    "            return x, x\n",
    "        # 1x68\n",
    "        ##### auto encoder is done, followed by decoder #####\n",
    "        # 1x68\n",
    "        x = self.fc_de(x)\n",
    "        # 1x4050\n",
    "        x = self.tanh(x)\n",
    "        # 1x4050\n",
    "        x = x.view(-1,50,8,8)\n",
    "        # 9*9*50\n",
    "        x = self.conv_de2(x)\n",
    "        # 17x17x50\n",
    "        x = self.leReLU(x)\n",
    "        # 17x17x50\n",
    "        x = self.conv_de1(x)\n",
    "        # 32x32x1\n",
    "        x = self.tanh(x)\n",
    "        x_de = x # this is the returned decoder\n",
    "        \n",
    "        return x_ae, x_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "nmi = normalized_mutual_info_score\n",
    "ari = adjusted_rand_score\n",
    "\n",
    "\n",
    "def acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEC:\n",
    "    \"\"\"The class for controlling the training process of DEC\"\"\"\n",
    "    def __init__(self,n_clusters,alpha=1.0):\n",
    "        self.n_clusters=n_clusters\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    @staticmethod\n",
    "    def target_distribution(q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return Variable((weight.t() / weight.sum(1)).t().data)\n",
    "    def logAccuracy(self,pred,label):\n",
    "        print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
    "          % (acc(label, pred), nmi(label, pred)))\n",
    "    \n",
    "    def pretrain(self,train_loader, test_loader, epochs):\n",
    "        \n",
    "        dec_ae = DEC_AE().cuda() #auto encoder\n",
    "        mseloss = nn.MSELoss()\n",
    "        optimizer = optim.SGD(dec_ae.parameters(),lr = 1, momentum=0.9)\n",
    "        best_acc = 0.0\n",
    "        for epoch in range(epochs):\n",
    "            dec_ae.train()\n",
    "            running_loss=0.0\n",
    "            x_eval = []\n",
    "            label_eval = []\n",
    "            for i,data in enumerate(train_loader):\n",
    "                x, label = data\n",
    "                x,label=Variable(x).cuda(),Variable(label).cuda()\n",
    "                optimizer.zero_grad()\n",
    "                x_ae,x_de = dec_ae(x)\n",
    "                loss = F.mse_loss(x_de,x,reduce=True) #mseloss(x_de,x) # so the aim is to minimize the reconstruct error\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                x_eval = x.data.cpu().numpy()\n",
    "                label_eval = label.data.cpu().numpy()\n",
    "                # print statistics\n",
    "                running_loss += loss.data.cpu().numpy()[0]\n",
    "                if i % 100 == 99:    # print every 2000 mini-batches\n",
    "                    print('[%d, %5d] loss: %.7f' %\n",
    "                          (epoch + 1, i + 1, running_loss / 100))\n",
    "                    #print('x_de:',x_de, x)\n",
    "                    running_loss = 0.0\n",
    "            #now we evaluate the accuracy with AE\n",
    "            dec_ae.eval()\n",
    "            print(x_eval.shape)\n",
    "            x_ae,_ = dec_ae(Variable(torch.from_numpy(x_eval)).cuda())\n",
    "            x_ae = x_ae.data.cpu().numpy()\n",
    "            print(label_eval.shape)\n",
    "            km = KMeans(n_clusters=len(np.unique(label_eval)), n_init=20, n_jobs=4)\n",
    "            y_pred = km.fit_predict(x_ae)\n",
    "            print(y_pred.shape)\n",
    "            print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
    "                      % (acc(label_eval, y_pred), nmi(label_eval, y_pred)))\n",
    "            if acc(label_eval, y_pred) > best_acc:\n",
    "                best_acc = acc(label_eval, y_pred)\n",
    "                torch.save(dec_ae,'bestModel'.format(best_acc))\n",
    "    def getTDistribution(self,x, clusterCenter):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "         q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\"\"\"\n",
    "        xe = torch.unsqueeze(x,1) - Variable(torch.from_numpy(clusterCenter.astype(np.float32))).cuda()\n",
    "        q = 1.0 / (1.0 + (torch.sum(torch.mul(xe,xe), 2) / self.alpha))\n",
    "        q = q ** (self.alpha + 1.0) / 2.0\n",
    "        q = (q.t() / torch.sum(q, 1)).t() #due to divison, we need to transpose q\n",
    "        return q\n",
    "    \n",
    "    def train(self,train_loader, test_loader, epochs):\n",
    "        \"\"\"This method will start training for DEC cluster\"\"\"\n",
    "        ct = time.time()\n",
    "        model = torch.load(\"bestModel\").cuda()\n",
    "        optimizer = optim.SGD(model.parameters(),lr = 0.00001, momentum=0.9)\n",
    "        print('Initializing cluster center with pre-trained weights')\n",
    "        km = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
    "        got_cluster_center = False\n",
    "        for epoch in range(epochs):\n",
    "            running_loss=0.0\n",
    "            for i,data in enumerate(train_loader):\n",
    "                x, label = data\n",
    "                x = Variable(x).cuda()\n",
    "                #step 1 - get cluster center from batch\n",
    "                if not got_cluster_center:                \n",
    "                    model.eval()\n",
    "                    y_pred_ae,_ = model(x)\n",
    "                    y_pred_ae = y_pred_ae.data.cpu().numpy()\n",
    "                    print('ae prediction', y_pred_ae,y_pred_ae.shape)\n",
    "                    y_pred = km.fit_predict(y_pred_ae) #seems we can only get a centre from batch\n",
    "                    print('cluster center:',km.cluster_centers_.shape)\n",
    "                    self.cluster_centers = km.cluster_centers_ #keep the cluster centers\n",
    "                    got_cluster_center = True\n",
    "                else:\n",
    "                    model.train()\n",
    "                    #now we start training with acquired cluster center\n",
    "                    feature_pred,_ = model(x)\n",
    "                    #output (batchSize,n_cluster)\n",
    "                    q =  self.getTDistribution(feature_pred, self.cluster_centers)\n",
    "                    #get target distribution\n",
    "                    p = self.target_distribution(q)\n",
    "                    #loss = kld(q,p)\n",
    "                    loss = F.kl_div(q,p)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss = running_loss + loss.data.cpu().numpy()[0]\n",
    "                    if i % 100 == 99:    # print every 2000 mini-batches\n",
    "                        print('[%d, %5d] loss: %.7f' %\n",
    "                              (epoch + 1, i + 1, running_loss / 100))\n",
    "                        running_loss = 0.0\n",
    "                        y_pred = np.argmax(q.data.cpu().numpy(),axis = 1)\n",
    "                        self.logAccuracy(y_pred,label.cpu().numpy())\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.0931961\n",
      "[1,   200] loss: 0.0362829\n",
      "[1,   300] loss: 0.0219840\n",
      "[1,   400] loss: 0.0169257\n",
      "[1,   500] loss: 0.0141389\n",
      "[1,   600] loss: 0.0124637\n",
      "(60000, 1, 28, 28)\n",
      "(60000,)\n",
      "(60000,)\n",
      "        |==>  acc: 0.5678,  nmi: 0.5325  <==|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/private/xlia/anaconda3/envs/py35/lib/python3.5/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type DEC_AE. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,   100] loss: 0.0113251\n",
      "[2,   200] loss: 0.0104553\n",
      "[2,   300] loss: 0.0098675\n",
      "[2,   400] loss: 0.0094984\n",
      "[2,   500] loss: 0.0090172\n",
      "[2,   600] loss: 0.0086762\n",
      "(60000, 1, 28, 28)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1518241554738/work/torch/lib/THC/generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5da2d4a57f7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1234\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-a7552184ae55>\u001b[0m in \u001b[0;36mpretrain\u001b[0;34m(self, train_loader, test_loader, epochs)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mlabel_eval\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mx_ae\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_ae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mx_ae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_ae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b96940e0106a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# 32x32x1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_ae1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m# 17x17x50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 282\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1518241554738/work/torch/lib/THC/generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "#now start training\n",
    "import random\n",
    "random.seed(1234)\n",
    "dec = DEC(10)\n",
    "dec.pretrain(train_loader, test_loader, 200)\n",
    "dec.train(train_loader, test_loader, 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
