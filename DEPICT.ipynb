{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.cluster import KMeans\n",
    "## load mnist dataset\n",
    "use_cuda = torch.cuda.is_available()\n",
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "#trans = transforms.Compose([transforms.Resize(32), transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "# if not exist, download mnist dataset\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)\n",
    "batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timingResult = {}\n",
    "def logTime(theName, currentTime):\n",
    "    if theName not in timingResult:\n",
    "        timingResult[theName] = time.time() - currentTime\n",
    "    else:\n",
    "        timingResult[theName] = timingResult[theName] + (time.time() - currentTime)\n",
    "    currentTime = time.time()\n",
    "    return currentTime\n",
    "\n",
    "def printTiming(name):\n",
    "    print('======== timing for {}: {} ======='.format(name,timingResult[name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEC_AE(nn.Module):\n",
    "    def __init__(self, num_classes, num_features):\n",
    "        super(DEC_AE,self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.fc1 = nn.Linear(28*28,500)\n",
    "        self.fc2 = nn.Linear(500,500)\n",
    "        self.fc3 = nn.Linear(500,2000)\n",
    "        self.fc4 = nn.Linear(2000,num_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_d1 = nn.Linear(500,28*28)\n",
    "        self.fc_d2 = nn.Linear(500,500)\n",
    "        self.fc_d3 = nn.Linear(2000,500)\n",
    "        self.fc_d4 = nn.Linear(num_features,2000)\n",
    "        self.alpha = 1.0\n",
    "        self.clusterCenter = nn.Parameter(torch.zeros(num_classes,num_features))\n",
    "        self.pretrainMode = True\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform(m.weight)\n",
    "\n",
    "    def setPretrain(self,mode):\n",
    "        \"\"\"To set training mode to pretrain or not, \n",
    "        so that it can control to run only the AE or AE+DECODER\"\"\"\n",
    "        self.pretrainMode = mode\n",
    "    \n",
    "    def updateClusterCenter(self, cc):\n",
    "        self.clusterCenter.data = torch.from_numpy(cc)\n",
    "        \n",
    "    def getTDistribution(self,x):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "         q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\"\"\"\n",
    "        xe = torch.unsqueeze(x.cuda(),1) - self.clusterCenter\n",
    "        q = 1.0 / (1.0 + (torch.sum(torch.mul(xe,xe), 2) / self.alpha))\n",
    "        q = q ** (self.alpha + 1.0) / 2.0\n",
    "        q = (q.t() / torch.sum(q, 1)).t() #due to divison, we need to transpose q\n",
    "        return q\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(-1, 1*28*28)\n",
    "        # 32x32x1\n",
    "        x = self.dropout(x)\n",
    "        # 32x32x1\n",
    "        x = self.fc1(x)\n",
    "        # 17x17x50\n",
    "        x = self.relu(x)\n",
    "        # 17x17x50\n",
    "        x = self.fc2(x)\n",
    "        # 17x17x50\n",
    "        x = self.relu(x)\n",
    "        # 9x9x50\n",
    "        x = self.fc3(x)\n",
    "        # 17x17x50\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        # 9x9x50\n",
    "        x_ae = x\n",
    "        #if not in pretrain mode, we only need encoder\n",
    "        if self.pretrainMode == False:\n",
    "            p = self.getTDistribution(x)\n",
    "            return x, p\n",
    "        # 1x68\n",
    "        ##### auto encoder is done, followed by decoder #####\n",
    "        # 1x68\n",
    "        x = self.fc_d4(x)\n",
    "        # 1x4050\n",
    "        x = self.relu(x)\n",
    "        # 1x4050\n",
    "        x = self.fc_d3(x)\n",
    "        # 1x4050\n",
    "        x = self.relu(x)\n",
    "        x = self.fc_d2(x)\n",
    "        # 1x4050\n",
    "        x = self.relu(x)\n",
    "        x = self.fc_d1(x)\n",
    "        x_de = x.view(-1,1,28,28)\n",
    "        # 1x4050\n",
    "        return x_ae, x_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "nmi = normalized_mutual_info_score\n",
    "ari = adjusted_rand_score\n",
    "\n",
    "\n",
    "def acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    \n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEC:\n",
    "    \"\"\"The class for controlling the training process of DEC\"\"\"\n",
    "    def __init__(self,n_clusters,alpha=1.0):\n",
    "        self.n_clusters=n_clusters\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    @staticmethod\n",
    "    def target_distribution(q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return Variable((weight.t() / weight.sum(1)).t().data)\n",
    "    def logAccuracy(self,pred,label):\n",
    "        print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
    "          % (acc(label, pred), nmi(label, pred)))\n",
    "    \n",
    "    def pretrain(self,train_loader, test_loader, epochs):\n",
    "        \n",
    "        dec_ae = DEC_AE(10,10).cuda() #auto encoder\n",
    "        mseloss = nn.MSELoss()\n",
    "        print([i for i in dec_ae.parameters()])\n",
    "        optimizer = optim.SGD(dec_ae.parameters(),lr = 1, momentum=0.9)\n",
    "        best_acc = 0.0\n",
    "        for epoch in range(epochs):\n",
    "            dec_ae.train()\n",
    "            running_loss=0.0\n",
    "            x_eval = []\n",
    "            label_eval = []\n",
    "            for i,data in enumerate(train_loader):\n",
    "                x, label = data\n",
    "                x,label=Variable(x).cuda(),Variable(label).cuda()\n",
    "                optimizer.zero_grad()\n",
    "                x_ae,x_de = dec_ae(x)\n",
    "                loss = F.mse_loss(x_de,x,reduce=True) #mseloss(x_de,x) # so the aim is to minimize the reconstruct error\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                x_eval = x.data.cpu().numpy()\n",
    "                label_eval = label.data.cpu().numpy()\n",
    "                # print statistics\n",
    "                running_loss += loss.data.cpu().numpy()[0]\n",
    "                if i % 100 == 99:    # print every 2000 mini-batches\n",
    "                    print('[%d, %5d] loss: %.7f' %\n",
    "                          (epoch + 1, i + 1, running_loss / 100))\n",
    "                    #print('x_de:',x_de, x)\n",
    "                    running_loss = 0.0\n",
    "            #now we evaluate the accuracy with AE\n",
    "            dec_ae.eval()\n",
    "            print(x_eval.shape)\n",
    "            x_ae,_ = dec_ae(Variable(torch.from_numpy(x_eval)).cuda())\n",
    "            x_ae = x_ae.data.cpu().numpy()\n",
    "            print(label_eval.shape)\n",
    "            km = KMeans(n_clusters=len(np.unique(label_eval)), n_init=20, n_jobs=4)\n",
    "            y_pred = km.fit_predict(x_ae)\n",
    "            print(y_pred.shape)\n",
    "            print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
    "                      % (acc(label_eval, y_pred), nmi(label_eval, y_pred)))\n",
    "            currentAcc = acc(label_eval, y_pred)\n",
    "            print(dec_ae.state_dict().keys())\n",
    "            if currentAcc > best_acc:                \n",
    "                torch.save(dec_ae,'bestModel'.format(best_acc))\n",
    "                best_acc = currentAcc\n",
    "                \n",
    "    def getTDistribution(self,x, clusterCenter):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "         q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\"\"\"\n",
    "        xe = torch.unsqueeze(x,1).cuda() - Variable(torch.from_numpy(clusterCenter.astype(np.float32))).cuda()\n",
    "        q = 1.0 / (1.0 + (torch.sum(torch.mul(xe,xe), 2) / self.alpha))\n",
    "        q = q ** (self.alpha + 1.0) / 2.0\n",
    "        q = (q.t() / torch.sum(q, 1)).t() #due to divison, we need to transpose q\n",
    "        return q\n",
    "    \n",
    "    def train(self,train_loader, test_loader, epochs):\n",
    "        \"\"\"This method will start training for DEC cluster\"\"\"\n",
    "        ct = time.time()\n",
    "        model = torch.load(\"bestModel\").cuda()\n",
    "        model.setPretrain(False)\n",
    "        optimizer = optim.SGD([\\\n",
    "             {'params': model.parameters()}, \\\n",
    "            ],lr = 0.01, momentum=0.9)\n",
    "        print('Initializing cluster center with pre-trained weights')\n",
    "        km = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
    "        got_cluster_center = False\n",
    "        for epoch in range(epochs):\n",
    "            running_loss=0.0\n",
    "            for i,data in enumerate(train_loader):\n",
    "                x, label = data\n",
    "                x = Variable(x).cuda()\n",
    "                #step 1 - get cluster center from batch\n",
    "                if not got_cluster_center:                \n",
    "                    model.eval()\n",
    "                    y_pred_ae,_ = model(x)\n",
    "                    y_pred_ae = y_pred_ae.data.cpu().numpy()\n",
    "                    print('ae prediction', y_pred_ae.shape)\n",
    "                    y_pred = km.fit_predict(y_pred_ae) #seems we can only get a centre from batch\n",
    "                    print('cluster center:',km.cluster_centers_.shape)\n",
    "                    self.cluster_centers = km.cluster_centers_ #keep the cluster centers\n",
    "                    model.updateClusterCenter(self.cluster_centers)\n",
    "                    print('model',model.state_dict())\n",
    "                    got_cluster_center = True\n",
    "                else:\n",
    "                    model.train()\n",
    "                    \n",
    "                    #now we start training with acquired cluster center\n",
    "                    feature_pred,_ = model(x)\n",
    "                    #output (batchSize,n_cluster)\n",
    "                    q =  self.getTDistribution(feature_pred, self.cluster_centers)\n",
    "                    #get target distribution\n",
    "                    p = self.target_distribution(q)\n",
    "                    #loss = kld(q,p)\n",
    "                    loss = F.kl_div(q,p)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss = running_loss + loss.data.cpu().numpy()[0]\n",
    "                    if i % 100 == 99:    # print every 2000 mini-batches\n",
    "                        print('[%d, %5d] loss: %.7f' %\n",
    "                              (epoch + 1, i + 1, running_loss / 100))\n",
    "                        running_loss = 0.0\n",
    "                        y_pred = np.argmax(q.data.cpu().numpy(),axis = 1)\n",
    "                        self.logAccuracy(y_pred,label.cpu().numpy())\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing cluster center with pre-trained weights\n",
      "ae prediction (100, 10)\n",
      "cluster center: (10, 10)\n",
      "model OrderedDict([('clusterCenter', \n",
      " 2.4902 -0.4237  0.5157 -3.1956  0.0619  2.5189 -0.0285  2.6575 -1.2623 -3.0487\n",
      " 0.4079 -0.2695 -4.3435  1.5530 -1.8933 -2.9724  1.4410 -1.6743  1.6491  0.1381\n",
      " 3.4787 -1.2061 -0.8199  3.3801 -0.5616  0.0545  0.4076 -2.0650 -2.5112 -0.3577\n",
      "-0.3212 -0.8037 -2.4732  2.0979  0.3179  0.1294  0.3345  2.3224 -0.0704  0.9107\n",
      " 1.2109  0.3397 -0.0121  0.3399 -0.9986 -0.9832 -2.5460  2.3720  3.5069  1.7639\n",
      "-0.5699 -3.7199 -3.9909 -2.3479  1.1653 -1.7321 -1.0598  2.2875  0.1055 -2.4138\n",
      " 4.0967  0.3714 -0.0314  0.1961  3.2683 -1.7943 -1.5224  0.0201 -3.6618  0.7124\n",
      " 2.1682 -2.6849 -1.2289  2.0534  0.2311  0.0047 -2.7016 -1.8598 -0.1003  0.6921\n",
      " 1.6858  0.8306  0.2000 -0.8013 -2.7713 -1.1341 -1.7663  0.5399 -1.2303 -1.1217\n",
      " 0.1444 -2.2742 -2.8888 -3.6588  1.8432 -0.8338  0.4286 -0.1951 -1.4972  3.1681\n",
      "[torch.FloatTensor of size 10x10]\n",
      "), ('fc1.weight', \n",
      "-3.4459e-02 -2.8136e-02  1.4196e-03  ...  -2.5092e-02 -2.9628e-02  4.2303e-02\n",
      "-9.8404e-03 -4.5796e-02  2.0935e-02  ...   4.6464e-02 -4.9951e-03 -1.9607e-02\n",
      "-5.8921e-02  6.4294e-03  4.6895e-03  ...  -2.8656e-02  5.2300e-02  4.9548e-02\n",
      "                ...                   ⋱                   ...                \n",
      " 4.3438e-02  4.4095e-02 -1.8166e-02  ...   4.2164e-02 -3.2457e-02 -6.5276e-02\n",
      " 2.4036e-03  2.4571e-03 -1.7475e-02  ...   5.2242e-02  3.6529e-02  2.8181e-02\n",
      " 5.8047e-02  4.7767e-02  6.7879e-02  ...   3.8856e-02 -8.4463e-03  8.0905e-03\n",
      "[torch.cuda.FloatTensor of size 500x784 (GPU 0)]\n",
      "), ('fc1.bias', \n",
      "1.00000e-02 *\n",
      "  2.8234\n",
      " -0.8037\n",
      "  0.3460\n",
      " -1.7041\n",
      "  2.2833\n",
      "  0.9659\n",
      "  2.9405\n",
      "  3.5796\n",
      " -0.1317\n",
      "  3.0346\n",
      " -0.3799\n",
      "  2.6611\n",
      " -2.4805\n",
      "  1.9861\n",
      "  2.5817\n",
      "  1.2067\n",
      "  4.2764\n",
      "  1.1129\n",
      " -3.1578\n",
      "  2.4687\n",
      " -2.3405\n",
      " -2.8097\n",
      " -0.9694\n",
      "  2.4790\n",
      "  1.1716\n",
      "  2.3131\n",
      " -1.3894\n",
      "  2.1975\n",
      "  1.3482\n",
      " -0.3414\n",
      "  1.2220\n",
      "  2.3368\n",
      "  0.5064\n",
      "  1.9156\n",
      "  1.3704\n",
      " -1.5373\n",
      "  4.0236\n",
      " -4.2780\n",
      " -3.1865\n",
      "  0.8880\n",
      "  3.5414\n",
      " -1.2085\n",
      " -2.0500\n",
      " -0.3613\n",
      "  3.8502\n",
      "  1.5326\n",
      "  4.0409\n",
      " -0.0987\n",
      "  1.4461\n",
      "  1.3510\n",
      " -0.7849\n",
      "  3.1982\n",
      " -2.1554\n",
      "  2.2196\n",
      " -0.5710\n",
      "  0.8172\n",
      "  0.6425\n",
      "  1.6882\n",
      " -0.9286\n",
      "  2.2293\n",
      "  0.9151\n",
      "  0.8312\n",
      "  2.5904\n",
      " -1.9913\n",
      "  2.7365\n",
      "  1.8461\n",
      "  0.2282\n",
      " -0.4103\n",
      "  3.1947\n",
      " -3.1240\n",
      " -2.9150\n",
      "  1.0505\n",
      "  1.2844\n",
      "  2.5684\n",
      " -1.9733\n",
      "  1.3585\n",
      " -1.4108\n",
      "  1.5529\n",
      "  2.6506\n",
      "  3.3650\n",
      "  0.7354\n",
      " -2.6495\n",
      "  4.1941\n",
      " -2.9131\n",
      "  2.7271\n",
      " -0.3688\n",
      " -1.7764\n",
      "  0.8637\n",
      " -1.6585\n",
      "  2.4653\n",
      " -0.4390\n",
      "  1.5833\n",
      " -2.1854\n",
      " -0.5646\n",
      " -1.5955\n",
      "  1.8656\n",
      "  0.7836\n",
      " -1.4339\n",
      "  0.2163\n",
      "  4.0004\n",
      " -3.8072\n",
      "  2.8442\n",
      "  0.0409\n",
      " -0.7855\n",
      " -0.7429\n",
      " -3.4047\n",
      "  2.2114\n",
      "  0.8164\n",
      " -1.7425\n",
      "  3.4460\n",
      "  1.9105\n",
      "  0.3236\n",
      " -0.4146\n",
      " -3.2102\n",
      "  2.4178\n",
      " -2.0285\n",
      " -1.0388\n",
      " -1.9774\n",
      "  3.7006\n",
      "  2.8915\n",
      " -1.6392\n",
      " -2.8894\n",
      "  1.6906\n",
      "  1.9759\n",
      " -0.7249\n",
      " -0.0439\n",
      "  3.0306\n",
      "  1.2507\n",
      " -0.6013\n",
      "  3.7987\n",
      "  1.3196\n",
      "  1.3835\n",
      " -2.7167\n",
      " -0.0061\n",
      "  4.2783\n",
      "  1.1373\n",
      "  0.2656\n",
      " -0.4759\n",
      "  2.2565\n",
      " -3.0553\n",
      " -2.1258\n",
      "  3.2459\n",
      " -0.7540\n",
      " -1.7434\n",
      "  0.7284\n",
      " -2.7714\n",
      "  2.4534\n",
      "  2.8581\n",
      " -1.8467\n",
      " -1.5651\n",
      " -3.0309\n",
      " -2.3354\n",
      "  3.2002\n",
      " -1.6894\n",
      "  3.5280\n",
      " -3.3024\n",
      " -0.8900\n",
      " -2.7350\n",
      " -2.9127\n",
      "  1.5480\n",
      "  1.4645\n",
      " -3.5109\n",
      "  1.0659\n",
      "  3.3434\n",
      "  0.9354\n",
      "  1.7338\n",
      "  3.8273\n",
      " -2.4822\n",
      " -1.5520\n",
      "  3.6173\n",
      "  0.9729\n",
      "  0.3802\n",
      " -1.0575\n",
      "  1.2836\n",
      " -4.2324\n",
      " -2.6698\n",
      "  2.5623\n",
      "  2.3028\n",
      " -3.1039\n",
      " -1.2196\n",
      "  3.4840\n",
      "  1.3855\n",
      "  2.3309\n",
      " -1.4664\n",
      " -3.3238\n",
      " -0.2360\n",
      "  0.9934\n",
      "  0.2569\n",
      " -1.5201\n",
      " -0.7272\n",
      " -0.0246\n",
      " -1.0985\n",
      " -3.7871\n",
      " -2.8301\n",
      "  0.8050\n",
      "  2.2353\n",
      "  3.2675\n",
      "  2.5451\n",
      "  0.0207\n",
      " -0.5502\n",
      "  2.5793\n",
      " -1.7245\n",
      "  3.8621\n",
      " -0.7995\n",
      " -2.4387\n",
      " -1.8999\n",
      "  0.4632\n",
      " -1.7401\n",
      "  1.2000\n",
      " -0.2119\n",
      "  1.0705\n",
      "  2.1005\n",
      "  3.0359\n",
      "  1.6341\n",
      " -0.9229\n",
      " -0.6690\n",
      " -3.4294\n",
      " -0.6382\n",
      " -2.8387\n",
      "  1.4501\n",
      " -3.7143\n",
      "  3.5375\n",
      " -2.1233\n",
      "  0.7745\n",
      " -0.6279\n",
      "  3.7418\n",
      "  2.2855\n",
      " -0.8993\n",
      "  2.0247\n",
      " -2.1176\n",
      " -0.7818\n",
      "  2.7802\n",
      "  3.6819\n",
      " -4.0475\n",
      "  3.0992\n",
      "  1.4672\n",
      " -2.6537\n",
      "  2.5437\n",
      "  1.0264\n",
      "  2.5853\n",
      " -0.2950\n",
      "  0.0844\n",
      " -0.9471\n",
      " -2.5061\n",
      " -0.7433\n",
      "  0.0620\n",
      " -0.2343\n",
      " -2.8775\n",
      " -1.9255\n",
      " -1.1013\n",
      "  2.4635\n",
      " -1.0673\n",
      "  1.9610\n",
      " -1.6025\n",
      " -1.1315\n",
      "  1.4654\n",
      " -1.1528\n",
      "  5.4071\n",
      "  1.6682\n",
      "  3.0301\n",
      "  3.5393\n",
      "  1.8138\n",
      "  0.6711\n",
      "  0.8210\n",
      " -1.0261\n",
      " -3.7421\n",
      " -2.4674\n",
      "  2.5835\n",
      " -2.2667\n",
      "  1.3771\n",
      "  1.4060\n",
      "  3.3207\n",
      "  2.6848\n",
      " -3.0443\n",
      "  1.9810\n",
      "  2.4075\n",
      "  3.0002\n",
      "  0.4918\n",
      " -3.1146\n",
      " -2.8761\n",
      " -0.7993\n",
      " -0.1305\n",
      "  1.6144\n",
      " -1.3351\n",
      " -0.3481\n",
      "  0.1860\n",
      "  0.6483\n",
      " -2.4109\n",
      "  0.2366\n",
      "  3.0600\n",
      " -0.0588\n",
      " -0.7023\n",
      "  1.5315\n",
      "  0.0118\n",
      "  2.0999\n",
      "  1.8273\n",
      "  0.9012\n",
      "  2.8037\n",
      " -0.9679\n",
      "  3.3582\n",
      "  1.0983\n",
      "  4.3509\n",
      "  2.8922\n",
      "  1.8563\n",
      "  1.9767\n",
      "  1.1969\n",
      "  2.9113\n",
      " -2.4499\n",
      " -2.2449\n",
      "  3.1555\n",
      " -3.1265\n",
      " -1.4629\n",
      "  2.4402\n",
      "  3.3873\n",
      " -2.1431\n",
      "  2.7408\n",
      " -1.6894\n",
      "  3.3390\n",
      " -0.2865\n",
      " -3.4871\n",
      "  2.4701\n",
      " -0.7126\n",
      " -0.3587\n",
      " -1.7536\n",
      "  1.3348\n",
      " -0.5329\n",
      "  4.0019\n",
      " -0.6347\n",
      " -2.4777\n",
      "  2.4686\n",
      " -0.7301\n",
      "  3.6958\n",
      "  1.3653\n",
      "  4.3433\n",
      " -3.1482\n",
      " -0.2235\n",
      "  0.6517\n",
      " -0.2464\n",
      "  1.8867\n",
      " -2.8688\n",
      " -0.9662\n",
      "  1.2927\n",
      "  3.6534\n",
      " -3.3552\n",
      " -0.9951\n",
      "  1.0855\n",
      "  1.8610\n",
      "  2.7413\n",
      " -3.4731\n",
      "  2.2054\n",
      " -2.6351\n",
      " -2.5400\n",
      "  3.2575\n",
      "  0.1249\n",
      " -0.1277\n",
      "  0.3692\n",
      " -1.8454\n",
      "  1.3562\n",
      "  1.2685\n",
      " -0.5260\n",
      " -3.1332\n",
      "  1.7670\n",
      " -0.3911\n",
      "  1.0520\n",
      "  4.4488\n",
      " -2.1454\n",
      "  2.1011\n",
      "  2.2616\n",
      "  0.9805\n",
      " -1.1949\n",
      " -1.3694\n",
      "  2.0764\n",
      " -0.4865\n",
      " -0.6861\n",
      " -1.5064\n",
      " -0.5966\n",
      " -2.8002\n",
      "  1.8626\n",
      "  0.9417\n",
      "  0.8653\n",
      "  0.3256\n",
      " -2.0490\n",
      "  2.2901\n",
      " -0.1124\n",
      " -2.4013\n",
      "  2.0220\n",
      " -1.4543\n",
      "  2.3066\n",
      " -2.8280\n",
      " -0.9653\n",
      " -2.7688\n",
      "  0.8177\n",
      " -0.2234\n",
      " -2.2340\n",
      "  1.7494\n",
      "  0.1500\n",
      " -3.4275\n",
      " -3.5446\n",
      "  0.8928\n",
      "  0.1567\n",
      "  0.8597\n",
      " -2.9389\n",
      " -3.5507\n",
      " -2.2197\n",
      "  2.4738\n",
      " -1.1652\n",
      " -0.0730\n",
      "  2.4491\n",
      " -0.6089\n",
      "  3.3028\n",
      "  2.9928\n",
      " -3.6715\n",
      " -0.8458\n",
      "  4.1764\n",
      "  3.2683\n",
      " -3.3514\n",
      "  1.1309\n",
      "  1.6509\n",
      " -1.7209\n",
      "  0.8062\n",
      "  2.8230\n",
      "  3.1109\n",
      " -1.4348\n",
      "  3.3240\n",
      " -0.5471\n",
      " -3.7602\n",
      "  2.5451\n",
      " -2.3534\n",
      "  5.2044\n",
      "  2.6549\n",
      "  2.2488\n",
      "  2.7023\n",
      "  1.1353\n",
      "  0.8603\n",
      " -3.0535\n",
      "  0.4637\n",
      " -2.3840\n",
      " -0.7771\n",
      " -3.7166\n",
      " -0.0466\n",
      "  2.5070\n",
      "  1.9708\n",
      "  2.1697\n",
      "  4.2733\n",
      " -2.3848\n",
      "  3.4838\n",
      "  2.7162\n",
      "  1.0208\n",
      " -0.0337\n",
      " -1.5472\n",
      " -0.3720\n",
      " -1.6083\n",
      " -0.6526\n",
      "  0.0005\n",
      " -1.4410\n",
      " -1.6551\n",
      "  1.4400\n",
      "  2.3929\n",
      "  0.4043\n",
      "  0.4769\n",
      " -1.0350\n",
      "  0.4088\n",
      "  1.3053\n",
      "  0.0049\n",
      "  3.0731\n",
      " -0.1892\n",
      "  4.8813\n",
      "  3.6321\n",
      "  2.8767\n",
      " -1.8227\n",
      "  1.5635\n",
      " -0.2826\n",
      "  3.6033\n",
      " -2.0811\n",
      "  2.4363\n",
      "  3.2960\n",
      " -0.9256\n",
      "  0.7874\n",
      "  0.8888\n",
      "  0.0409\n",
      " -2.7257\n",
      " -1.2320\n",
      "  3.6551\n",
      "  1.2688\n",
      "  2.8516\n",
      "  2.0706\n",
      "  0.4687\n",
      "  0.9092\n",
      " -1.5217\n",
      "  1.4928\n",
      " -2.2667\n",
      " -2.2979\n",
      "  3.8357\n",
      "  0.7471\n",
      " -0.5091\n",
      " -0.4558\n",
      "  0.3902\n",
      " -2.8235\n",
      " -2.8099\n",
      "  2.7190\n",
      "[torch.cuda.FloatTensor of size 500 (GPU 0)]\n",
      "), ('fc2.weight', \n",
      " 6.9894e-02  5.2259e-02  5.9807e-02  ...  -2.9445e-02  2.4077e-04 -6.8403e-02\n",
      "-6.3564e-02 -5.2063e-02 -6.2738e-02  ...   1.8841e-02 -7.1174e-02 -1.5074e-02\n",
      "-1.1820e-02  6.4870e-02  4.1675e-02  ...   5.7297e-02  7.6928e-03  7.3996e-02\n",
      "                ...                   ⋱                   ...                \n",
      "-6.8794e-02  3.4095e-02 -5.8659e-02  ...  -5.9283e-02 -2.1225e-02  6.2602e-02\n",
      "-8.8914e-02  6.3474e-03 -4.7291e-02  ...  -2.3933e-02 -3.5552e-02 -3.3860e-02\n",
      "-6.2443e-02 -3.5046e-02  4.6522e-02  ...   4.2118e-02 -3.0764e-03  3.0147e-02\n",
      "[torch.cuda.FloatTensor of size 500x500 (GPU 0)]\n",
      "), ('fc2.bias', \n",
      "1.00000e-02 *\n",
      "  2.9086\n",
      " -4.2963\n",
      " -0.4715\n",
      " -2.3779\n",
      " -0.6972\n",
      " -3.6178\n",
      "  4.0597\n",
      " -0.5549\n",
      " -0.8413\n",
      "  0.3157\n",
      " -0.8561\n",
      " -1.9237\n",
      "  0.7666\n",
      " -4.6122\n",
      "  0.4085\n",
      " -1.4442\n",
      "  3.2592\n",
      " -3.0626\n",
      " -2.1083\n",
      " -2.5071\n",
      "  0.5866\n",
      "  2.0381\n",
      " -0.7191\n",
      "  3.8694\n",
      "  1.4732\n",
      " -0.0152\n",
      " -2.5878\n",
      "  4.3878\n",
      " -4.4438\n",
      "  0.8860\n",
      "  1.2379\n",
      "  1.9166\n",
      " -4.7766\n",
      " -3.3998\n",
      " -0.1906\n",
      "  3.6264\n",
      " -0.7041\n",
      "  3.4364\n",
      "  2.8115\n",
      "  3.4436\n",
      "  5.1684\n",
      " -1.9190\n",
      "  3.3011\n",
      "  1.2382\n",
      " -3.1726\n",
      "  1.0710\n",
      "  2.9185\n",
      "  0.0996\n",
      " -3.9533\n",
      " -3.9464\n",
      " -2.3127\n",
      " -3.2844\n",
      "  2.6686\n",
      "  3.8052\n",
      " -2.8765\n",
      " -0.9224\n",
      " -3.5811\n",
      "  0.6007\n",
      " -3.4420\n",
      " -3.6774\n",
      "  1.1016\n",
      "  3.6088\n",
      " -4.0712\n",
      "  0.5944\n",
      " -3.0654\n",
      " -2.7721\n",
      "  1.4292\n",
      "  0.0424\n",
      " -2.6871\n",
      " -3.8233\n",
      "  1.0829\n",
      " -1.1664\n",
      " -2.7383\n",
      "  4.0336\n",
      " -1.0600\n",
      " -2.9382\n",
      "  4.8741\n",
      " -1.0337\n",
      " -1.1672\n",
      " -4.4752\n",
      " -0.3851\n",
      "  1.1504\n",
      " -2.2125\n",
      "  1.3778\n",
      " -1.2744\n",
      " -0.1912\n",
      "  0.6018\n",
      " -2.3773\n",
      " -2.0029\n",
      " -2.3239\n",
      "  4.1812\n",
      "  5.5425\n",
      "  0.9132\n",
      "  3.8382\n",
      "  4.3576\n",
      "  4.3587\n",
      "  1.1608\n",
      " -1.9963\n",
      " -0.1506\n",
      " -0.0630\n",
      " -1.8097\n",
      "  0.5002\n",
      " -1.3554\n",
      "  3.3144\n",
      " -3.7319\n",
      "  3.1282\n",
      "  2.3157\n",
      "  3.7735\n",
      "  3.2700\n",
      "  3.3972\n",
      " -1.5667\n",
      " -3.8729\n",
      "  4.2706\n",
      " -1.6273\n",
      " -1.7152\n",
      "  1.3848\n",
      " -3.9676\n",
      "  3.3887\n",
      "  3.6747\n",
      " -3.7191\n",
      "  3.4553\n",
      "  3.8795\n",
      "  4.6951\n",
      " -1.9968\n",
      "  2.8548\n",
      " -3.5166\n",
      "  3.7710\n",
      "  1.0543\n",
      " -2.2181\n",
      " -3.6113\n",
      " -3.8210\n",
      " -3.8562\n",
      "  1.2753\n",
      "  2.0242\n",
      "  1.7442\n",
      " -0.5594\n",
      "  4.4910\n",
      "  1.5670\n",
      "  2.5697\n",
      " -3.8672\n",
      " -2.1023\n",
      "  3.1219\n",
      "  1.9932\n",
      " -1.4874\n",
      "  2.8637\n",
      "  1.9312\n",
      " -2.0424\n",
      " -0.0203\n",
      " -1.5681\n",
      " -1.8032\n",
      "  2.7308\n",
      "  2.7747\n",
      "  2.8506\n",
      "  1.8651\n",
      "  0.3306\n",
      "  0.4254\n",
      " -0.0321\n",
      " -4.0759\n",
      "  0.9314\n",
      "  4.0143\n",
      "  0.7199\n",
      " -1.0583\n",
      " -1.3783\n",
      "  1.0152\n",
      " -0.9556\n",
      " -4.4749\n",
      "  0.7369\n",
      " -1.2133\n",
      "  2.0611\n",
      "  2.0882\n",
      " -1.2671\n",
      " -1.5027\n",
      "  1.8261\n",
      "  2.1608\n",
      " -4.1451\n",
      " -2.4588\n",
      "  3.7348\n",
      "  1.3713\n",
      "  1.1252\n",
      " -0.5725\n",
      "  1.3563\n",
      " -3.8862\n",
      " -3.1523\n",
      "  0.5767\n",
      "  1.0367\n",
      " -2.9004\n",
      " -3.7499\n",
      " -2.9674\n",
      "  1.6312\n",
      "  3.6246\n",
      "  5.5728\n",
      " -0.1590\n",
      "  2.3796\n",
      "  4.1964\n",
      "  3.3836\n",
      " -3.2238\n",
      "  3.1334\n",
      " -4.6825\n",
      " -0.6733\n",
      "  3.8216\n",
      " -1.1069\n",
      "  0.0347\n",
      " -2.7198\n",
      " -3.2479\n",
      " -1.0713\n",
      "  4.0215\n",
      "  2.5067\n",
      " -3.2538\n",
      "  0.7787\n",
      "  0.4480\n",
      "  0.7180\n",
      " -2.2621\n",
      "  3.4962\n",
      "  1.1988\n",
      " -1.6593\n",
      "  0.0974\n",
      " -0.9868\n",
      "  1.2481\n",
      "  4.1808\n",
      "  2.7137\n",
      " -0.8804\n",
      "  3.7716\n",
      " -2.9933\n",
      " -3.4764\n",
      "  4.5314\n",
      "  0.8791\n",
      "  3.7753\n",
      "  3.0454\n",
      "  1.6074\n",
      "  0.1243\n",
      " -1.0056\n",
      " -1.6818\n",
      " -1.2494\n",
      "  2.7498\n",
      " -1.0190\n",
      "  0.6430\n",
      " -1.7495\n",
      " -0.3247\n",
      "  1.2528\n",
      "  2.6155\n",
      " -1.5707\n",
      "  5.2939\n",
      " -2.6130\n",
      "  0.6718\n",
      " -3.2729\n",
      "  1.6334\n",
      "  4.2577\n",
      "  3.0669\n",
      " -0.6889\n",
      " -1.0917\n",
      "  4.2077\n",
      " -4.3695\n",
      " -2.6480\n",
      "  1.6209\n",
      "  3.3565\n",
      " -1.8544\n",
      "  3.1464\n",
      "  3.9620\n",
      "  0.6472\n",
      " -1.9976\n",
      "  0.4066\n",
      " -2.9900\n",
      "  3.9151\n",
      " -4.0129\n",
      " -2.2332\n",
      " -0.5931\n",
      "  0.9397\n",
      " -0.8286\n",
      "  1.4853\n",
      "  0.9277\n",
      "  1.6957\n",
      " -4.0171\n",
      "  3.3143\n",
      " -2.7101\n",
      "  1.1207\n",
      "  3.0563\n",
      " -0.7913\n",
      " -1.8819\n",
      " -2.7368\n",
      "  2.9141\n",
      "  1.6414\n",
      "  2.8663\n",
      "  2.1893\n",
      "  1.4874\n",
      " -3.1261\n",
      " -3.9870\n",
      "  1.2684\n",
      " -3.6548\n",
      " -0.7094\n",
      " -0.2390\n",
      "  0.2169\n",
      " -4.3541\n",
      " -2.9110\n",
      "  3.5117\n",
      "  1.0947\n",
      "  0.4108\n",
      " -3.7031\n",
      "  0.3406\n",
      "  0.8116\n",
      "  3.1646\n",
      " -1.4804\n",
      "  2.8504\n",
      "  1.3672\n",
      "  0.0856\n",
      "  1.1025\n",
      " -0.4273\n",
      "  2.3694\n",
      "  4.2282\n",
      " -3.2779\n",
      " -3.5227\n",
      "  3.2894\n",
      " -3.7558\n",
      " -1.0792\n",
      " -2.5095\n",
      "  5.1337\n",
      "  0.2751\n",
      "  1.4506\n",
      " -4.8092\n",
      "  0.5493\n",
      " -1.1281\n",
      " -3.5018\n",
      "  2.4388\n",
      "  3.9123\n",
      "  2.1850\n",
      " -2.7393\n",
      " -3.9125\n",
      " -0.5457\n",
      " -2.7981\n",
      "  0.3833\n",
      " -0.0004\n",
      " -0.4241\n",
      "  2.7315\n",
      "  1.4849\n",
      " -0.1992\n",
      " -0.6916\n",
      "  0.8292\n",
      " -0.8357\n",
      " -2.9588\n",
      " -3.1935\n",
      "  0.6433\n",
      "  3.6257\n",
      " -1.6213\n",
      "  3.1893\n",
      "  2.3207\n",
      " -3.9118\n",
      "  2.2914\n",
      " -4.5430\n",
      "  1.2524\n",
      "  0.6516\n",
      "  0.2572\n",
      " -3.3094\n",
      " -3.0735\n",
      "  0.2458\n",
      " -2.2530\n",
      " -0.4333\n",
      " -2.4912\n",
      " -3.9652\n",
      "  0.1320\n",
      "  1.3676\n",
      " -0.4765\n",
      " -1.9732\n",
      "  0.4797\n",
      "  1.7378\n",
      " -1.9023\n",
      "  4.3144\n",
      "  1.0250\n",
      "  1.6560\n",
      " -1.0793\n",
      "  1.1607\n",
      "  3.3716\n",
      " -2.7585\n",
      "  0.5069\n",
      "  3.1493\n",
      "  3.2551\n",
      " -3.0720\n",
      " -0.2832\n",
      " -1.0495\n",
      "  3.6541\n",
      "  2.9259\n",
      " -4.1352\n",
      " -1.7979\n",
      "  1.7164\n",
      "  2.4224\n",
      "  3.1560\n",
      "  4.0665\n",
      " -1.9707\n",
      "  2.5537\n",
      "  2.9601\n",
      " -1.2026\n",
      "  1.5430\n",
      " -4.3528\n",
      "  3.1061\n",
      "  3.3371\n",
      "  3.7804\n",
      "  2.8327\n",
      " -3.5162\n",
      "  4.6160\n",
      "  3.8199\n",
      " -3.3159\n",
      "  1.8823\n",
      "  3.9705\n",
      "  3.0234\n",
      " -3.8648\n",
      "  1.0744\n",
      "  3.8296\n",
      " -0.2331\n",
      "  2.4381\n",
      " -3.1762\n",
      " -0.3805\n",
      " -1.3935\n",
      " -3.3965\n",
      "  1.8549\n",
      "  4.2269\n",
      " -0.6552\n",
      " -1.0821\n",
      " -3.7091\n",
      "  1.9678\n",
      " -1.0419\n",
      " -0.1250\n",
      "  0.9087\n",
      "  2.0552\n",
      "  3.6169\n",
      " -0.5560\n",
      " -0.4300\n",
      "  3.9955\n",
      " -3.7784\n",
      "  4.2510\n",
      "  0.7261\n",
      " -3.4570\n",
      "  1.0275\n",
      "  1.7581\n",
      "  1.8353\n",
      "  3.5021\n",
      " -2.6860\n",
      "  3.7294\n",
      "  0.3927\n",
      "  1.9917\n",
      " -3.1502\n",
      " -0.3091\n",
      "  4.1835\n",
      "  0.3891\n",
      "  0.2821\n",
      "  2.3658\n",
      "  1.4110\n",
      "  2.0676\n",
      "  0.4088\n",
      "  4.2738\n",
      " -3.4069\n",
      "  0.4025\n",
      "  0.2478\n",
      " -0.3557\n",
      "  2.4735\n",
      " -1.9410\n",
      "  0.2300\n",
      "  1.4626\n",
      " -0.9715\n",
      " -0.1138\n",
      " -3.2064\n",
      " -2.2262\n",
      " -4.4946\n",
      " -3.8449\n",
      "  2.7394\n",
      "  2.4609\n",
      "  0.8389\n",
      "  3.2019\n",
      " -3.9766\n",
      " -1.3495\n",
      " -0.8961\n",
      " -4.0922\n",
      " -1.5636\n",
      "  4.9704\n",
      "  1.2882\n",
      "  5.4565\n",
      "  3.3309\n",
      " -0.6759\n",
      " -2.2687\n",
      " -0.0624\n",
      "  0.5430\n",
      " -0.5127\n",
      "  0.5303\n",
      "  3.4174\n",
      " -0.8030\n",
      " -3.4101\n",
      " -1.1093\n",
      "  0.0360\n",
      " -0.2354\n",
      "  2.6502\n",
      " -0.7397\n",
      "  5.4269\n",
      " -4.0382\n",
      " -0.0226\n",
      " -2.5072\n",
      " -3.8698\n",
      " -1.9956\n",
      " -2.0047\n",
      "  2.1588\n",
      " -3.0219\n",
      "  1.0766\n",
      "  0.5191\n",
      "  1.7221\n",
      "[torch.cuda.FloatTensor of size 500 (GPU 0)]\n",
      "), ('fc3.weight', \n",
      "-1.4511e-02  3.2827e-02 -3.8984e-03  ...  -1.6497e-02 -4.3157e-03 -1.9852e-02\n",
      " 1.2024e-02  2.8109e-02  1.0829e-02  ...   4.5314e-02 -3.1145e-03 -1.9206e-02\n",
      " 3.9849e-02  4.4545e-02 -2.3396e-02  ...  -3.5007e-02  4.2294e-03  3.9382e-02\n",
      "                ...                   ⋱                   ...                \n",
      " 1.4713e-02 -1.2935e-02 -1.9949e-02  ...   4.2371e-02 -5.9933e-03  3.4200e-02\n",
      "-2.1060e-02  1.8539e-02 -3.9110e-02  ...  -1.4991e-03 -2.0153e-02  3.5673e-02\n",
      "-4.0877e-02 -1.1277e-02  1.2580e-03  ...   2.2223e-02 -4.2313e-02  3.7768e-02\n",
      "[torch.cuda.FloatTensor of size 2000x500 (GPU 0)]\n",
      "), ('fc3.bias', \n",
      "1.00000e-02 *\n",
      " -0.4097\n",
      " -2.5141\n",
      " -1.9862\n",
      "    ⋮   \n",
      "  4.1831\n",
      "  0.6245\n",
      " -0.2218\n",
      "[torch.cuda.FloatTensor of size 2000 (GPU 0)]\n",
      "), ('fc4.weight', \n",
      " 1.7157e-03 -5.8995e-02 -1.2223e-02  ...  -7.6349e-02  2.9167e-02  8.4576e-02\n",
      "-1.8031e-02 -3.0008e-02  2.0620e-02  ...   3.0552e-02  3.1318e-03  2.2882e-02\n",
      "-2.6909e-02  2.7598e-02 -4.8484e-02  ...   4.0164e-02  4.5846e-02  9.7624e-03\n",
      "                ...                   ⋱                   ...                \n",
      " 7.9457e-02  2.9001e-02 -2.4742e-02  ...   1.1027e-02  5.3991e-02  9.1310e-02\n",
      "-2.0219e-02 -5.9606e-02  6.4388e-02  ...  -8.5526e-03 -3.0352e-02 -1.6381e-02\n",
      "-3.7223e-02 -8.0752e-03  2.6453e-02  ...  -2.9136e-02 -4.7024e-02  1.3491e-02\n",
      "[torch.cuda.FloatTensor of size 10x2000 (GPU 0)]\n",
      "), ('fc4.bias', \n",
      "1.00000e-02 *\n",
      "  1.9646\n",
      "  3.8004\n",
      "  1.9652\n",
      " -5.1487\n",
      "  1.2586\n",
      "  3.2700\n",
      " -0.2894\n",
      "  7.1751\n",
      "  4.2328\n",
      " -1.1585\n",
      "[torch.cuda.FloatTensor of size 10 (GPU 0)]\n",
      "), ('fc_d1.weight', \n",
      "-1.5049e-02 -6.0292e-02  5.7135e-03  ...  -2.7525e-02  1.8216e-02  8.2795e-03\n",
      " 7.6430e-03  1.8834e-02 -8.9211e-03  ...   7.8590e-03 -4.7980e-02  4.0300e-02\n",
      " 1.2189e-02  7.4362e-03  4.5839e-02  ...  -1.9613e-02  3.3069e-02 -1.7287e-02\n",
      "                ...                   ⋱                   ...                \n",
      " 3.3069e-02  5.6629e-02 -4.7173e-02  ...   1.1106e-02  4.7477e-02  3.6906e-02\n",
      "-2.6133e-03  5.0075e-02 -2.7546e-03  ...   1.2302e-02 -3.8118e-02 -3.1895e-02\n",
      "-6.4388e-02 -5.9592e-02 -1.2924e-02  ...  -7.1924e-02  4.0001e-02 -2.6012e-02\n",
      "[torch.cuda.FloatTensor of size 784x500 (GPU 0)]\n",
      "), ('fc_d1.bias', \n",
      "-0.0722\n",
      "-0.0640\n",
      "-0.0555\n",
      "-0.0994\n",
      "-0.0588\n",
      "-0.0716\n",
      "-0.0722\n",
      "-0.0538\n",
      "-0.0750\n",
      "-0.0988\n",
      "-0.0689\n",
      "-0.0650\n",
      "-0.0784\n",
      "-0.0879\n",
      "-0.1227\n",
      "-0.1017\n",
      "-0.0556\n",
      "-0.0971\n",
      "-0.1017\n",
      "-0.0664\n",
      "-0.0973\n",
      "-0.0442\n",
      "-0.1142\n",
      "-0.1053\n",
      "-0.1137\n",
      "-0.0678\n",
      "-0.0399\n",
      "-0.0765\n",
      "-0.1132\n",
      "-0.0681\n",
      "-0.0484\n",
      "-0.0449\n",
      "-0.1189\n",
      "-0.0569\n",
      "-0.1057\n",
      "-0.0899\n",
      "-0.0670\n",
      "-0.1344\n",
      "-0.0715\n",
      "-0.0766\n",
      "-0.0802\n",
      "-0.0562\n",
      "-0.0972\n",
      "-0.0798\n",
      "-0.0521\n",
      "-0.0773\n",
      "-0.0799\n",
      "-0.0800\n",
      "-0.1204\n",
      "-0.0563\n",
      "-0.0875\n",
      "-0.0829\n",
      "-0.0769\n",
      "-0.0935\n",
      "-0.0808\n",
      "-0.0700\n",
      "-0.1092\n",
      "-0.1027\n",
      "-0.0302\n",
      "-0.0757\n",
      "-0.1076\n",
      "-0.0670\n",
      "-0.0653\n",
      "-0.0752\n",
      "-0.0459\n",
      "-0.1142\n",
      "-0.0476\n",
      "-0.0946\n",
      "-0.0872\n",
      "-0.1303\n",
      "-0.0600\n",
      "-0.1175\n",
      "-0.0482\n",
      "-0.0513\n",
      "-0.0957\n",
      "-0.1317\n",
      "-0.1133\n",
      "-0.0245\n",
      "-0.0647\n",
      "-0.1273\n",
      "-0.0872\n",
      "-0.0785\n",
      "-0.0947\n",
      "-0.0926\n",
      "-0.0520\n",
      "-0.0917\n",
      "-0.0594\n",
      "-0.0843\n",
      "-0.1085\n",
      "-0.0997\n",
      "-0.1191\n",
      "-0.0903\n",
      "-0.1073\n",
      "-0.0947\n",
      "-0.0279\n",
      "-0.0636\n",
      "-0.0518\n",
      "-0.1007\n",
      "-0.0354\n",
      "-0.0440\n",
      "-0.0538\n",
      "-0.0368\n",
      "-0.0645\n",
      "-0.0877\n",
      "-0.1165\n",
      "-0.0405\n",
      "-0.0963\n",
      "-0.0920\n",
      "-0.1152\n",
      "-0.0651\n",
      "-0.1048\n",
      "-0.0659\n",
      "-0.0496\n",
      "-0.0875\n",
      "-0.0943\n",
      "-0.0688\n",
      "-0.0971\n",
      "-0.0677\n",
      "-0.0642\n",
      "-0.1260\n",
      "-0.0832\n",
      "-0.0791\n",
      "-0.0642\n",
      "-0.0320\n",
      "-0.0414\n",
      "-0.0344\n",
      "-0.0499\n",
      "-0.0588\n",
      "-0.0568\n",
      "-0.1035\n",
      "-0.1049\n",
      "-0.0366\n",
      "-0.0710\n",
      "-0.0638\n",
      "-0.0544\n",
      "-0.1128\n",
      "-0.0809\n",
      "-0.1381\n",
      "-0.0719\n",
      "-0.0919\n",
      "-0.0961\n",
      "-0.0508\n",
      "-0.1047\n",
      "-0.1096\n",
      "-0.0948\n",
      "-0.0861\n",
      "-0.0962\n",
      "-0.0921\n",
      "-0.0547\n",
      "-0.0522\n",
      "-0.0778\n",
      "-0.0367\n",
      "-0.0194\n",
      "-0.0014\n",
      "-0.0128\n",
      "-0.0268\n",
      "-0.0139\n",
      "-0.0380\n",
      "-0.0327\n",
      "-0.0977\n",
      "-0.0837\n",
      "-0.0734\n",
      "-0.0630\n",
      "-0.0766\n",
      "-0.0853\n",
      "-0.1054\n",
      "-0.0997\n",
      "-0.0638\n",
      "-0.0664\n",
      "-0.0574\n",
      "-0.0930\n",
      "-0.0493\n",
      "-0.0886\n",
      "-0.1081\n",
      "-0.0432\n",
      "-0.0638\n",
      "-0.0437\n",
      " 0.0054\n",
      "-0.0032\n",
      " 0.0208\n",
      "-0.0110\n",
      " 0.0303\n",
      " 0.0020\n",
      "-0.0154\n",
      "-0.0028\n",
      "-0.0326\n",
      "-0.0128\n",
      "-0.0102\n",
      "-0.0485\n",
      "-0.0500\n",
      "-0.0991\n",
      "-0.0782\n",
      "-0.0819\n",
      "-0.1075\n",
      "-0.0908\n",
      "-0.0351\n",
      "-0.0384\n",
      "-0.0759\n",
      "-0.0722\n",
      "-0.0679\n",
      "-0.1092\n",
      "-0.0465\n",
      "-0.0294\n",
      "-0.0203\n",
      "-0.0456\n",
      " 0.0012\n",
      " 0.0033\n",
      " 0.0170\n",
      "-0.0303\n",
      " 0.0405\n",
      " 0.0303\n",
      "-0.0268\n",
      " 0.0126\n",
      "-0.0347\n",
      "-0.0012\n",
      "-0.0226\n",
      "-0.0078\n",
      "-0.0438\n",
      "-0.0890\n",
      "-0.0602\n",
      "-0.1106\n",
      "-0.0460\n",
      "-0.0777\n",
      "-0.0785\n",
      "-0.0852\n",
      "-0.0478\n",
      "-0.0819\n",
      "-0.1241\n",
      "-0.0638\n",
      "-0.0445\n",
      "-0.0729\n",
      "-0.0127\n",
      "-0.0511\n",
      "-0.0050\n",
      "-0.0615\n",
      "-0.0249\n",
      " 0.0015\n",
      " 0.0231\n",
      "-0.0178\n",
      " 0.0210\n",
      "-0.0099\n",
      "-0.0280\n",
      " 0.0470\n",
      "-0.0275\n",
      "-0.0380\n",
      "-0.0311\n",
      "-0.0280\n",
      "-0.0918\n",
      "-0.0443\n",
      "-0.0438\n",
      "-0.0773\n",
      "-0.0738\n",
      "-0.1253\n",
      "-0.1183\n",
      "-0.0598\n",
      "-0.0573\n",
      "-0.1146\n",
      "-0.0673\n",
      "-0.1181\n",
      "-0.0373\n",
      "-0.0567\n",
      "-0.0592\n",
      "-0.0168\n",
      "-0.0075\n",
      " 0.0005\n",
      "-0.0403\n",
      "-0.0105\n",
      "-0.0724\n",
      "-0.0071\n",
      "-0.0109\n",
      " 0.0326\n",
      "-0.0192\n",
      "-0.0601\n",
      "-0.0833\n",
      "-0.0902\n",
      "-0.0344\n",
      "-0.0515\n",
      "-0.0919\n",
      "-0.0740\n",
      "-0.0598\n",
      "-0.0642\n",
      "-0.0535\n",
      "-0.0629\n",
      "-0.0338\n",
      "-0.0478\n",
      "-0.0297\n",
      "-0.0113\n",
      "-0.0146\n",
      "-0.0343\n",
      "-0.0315\n",
      " 0.0084\n",
      " 0.0015\n",
      "-0.0472\n",
      "-0.0680\n",
      "-0.0900\n",
      "-0.0352\n",
      "-0.0608\n",
      " 0.0234\n",
      "-0.0008\n",
      "-0.0384\n",
      "-0.0737\n",
      "-0.0133\n",
      " 0.0011\n",
      "-0.0230\n",
      "-0.0558\n",
      "-0.1046\n",
      "-0.0821\n",
      "-0.0989\n",
      "-0.0507\n",
      "-0.0744\n",
      "-0.1278\n",
      "-0.0868\n",
      "-0.0887\n",
      "-0.0844\n",
      "-0.0432\n",
      "-0.0317\n",
      "-0.0725\n",
      " 0.0325\n",
      "-0.0386\n",
      " 0.0271\n",
      " 0.0052\n",
      "-0.0760\n",
      "-0.0763\n",
      "-0.0203\n",
      " 0.0154\n",
      " 0.0193\n",
      "-0.0115\n",
      " 0.0132\n",
      " 0.0147\n",
      "-0.0228\n",
      "-0.0296\n",
      "-0.1251\n",
      "-0.0985\n",
      "-0.1209\n",
      "-0.1241\n",
      "-0.0634\n",
      "-0.0457\n",
      "-0.0507\n",
      "-0.0430\n",
      "-0.0892\n",
      "-0.0574\n",
      "-0.0941\n",
      "-0.0970\n",
      "-0.0173\n",
      " 0.0129\n",
      "-0.0064\n",
      " 0.0104\n",
      " 0.0015\n",
      "-0.0217\n",
      "-0.0460\n",
      "-0.0216\n",
      "-0.0054\n",
      "-0.0405\n",
      " 0.0150\n",
      " 0.0047\n",
      "-0.0287\n",
      "-0.0375\n",
      " 0.0048\n",
      "-0.0212\n",
      "-0.0884\n",
      "-0.0564\n",
      "-0.1003\n",
      "-0.0356\n",
      "-0.1252\n",
      "-0.0542\n",
      "-0.0981\n",
      "-0.0835\n",
      "-0.1131\n",
      "-0.0606\n",
      "-0.0731\n",
      "-0.0580\n",
      "-0.0370\n",
      "-0.0115\n",
      "-0.0700\n",
      "-0.0350\n",
      "-0.0451\n",
      "-0.0455\n",
      "-0.0201\n",
      "-0.0099\n",
      "-0.0532\n",
      " 0.0485\n",
      "-0.0167\n",
      " 0.0109\n",
      "-0.0033\n",
      " 0.0093\n",
      "-0.0688\n",
      "-0.0342\n",
      "-0.0615\n",
      "-0.0387\n",
      "-0.0594\n",
      "-0.0865\n",
      "-0.0719\n",
      "-0.0980\n",
      "-0.0724\n",
      "-0.0841\n",
      "-0.0856\n",
      "-0.1070\n",
      "-0.0509\n",
      "-0.0793\n",
      "-0.0497\n",
      "-0.0443\n",
      "-0.0397\n",
      "-0.0053\n",
      " 0.0138\n",
      "-0.0230\n",
      "-0.0441\n",
      "-0.0346\n",
      "-0.0338\n",
      "-0.0255\n",
      " 0.0223\n",
      "-0.0165\n",
      "-0.0210\n",
      " 0.0403\n",
      "-0.0584\n",
      "-0.0115\n",
      "-0.0679\n",
      "-0.0629\n",
      "-0.0626\n",
      "-0.0831\n",
      "-0.1160\n",
      "-0.0763\n",
      "-0.0884\n",
      "-0.0988\n",
      "-0.1059\n",
      "-0.0854\n",
      "-0.1083\n",
      "-0.0111\n",
      "-0.0106\n",
      "-0.0009\n",
      "-0.0219\n",
      " 0.0087\n",
      "-0.0494\n",
      " 0.0417\n",
      " 0.0093\n",
      " 0.0133\n",
      "-0.0187\n",
      " 0.0248\n",
      " 0.0274\n",
      "-0.0083\n",
      " 0.0227\n",
      " 0.0026\n",
      "-0.0162\n",
      "-0.0715\n",
      "-0.0923\n",
      "-0.1075\n",
      "-0.0918\n",
      "-0.1248\n",
      "-0.0935\n",
      "-0.0481\n",
      "-0.0668\n",
      "-0.0708\n",
      "-0.0660\n",
      "-0.0386\n",
      "-0.0181\n",
      "-0.0197\n",
      " 0.0062\n",
      "-0.0179\n",
      " 0.0355\n",
      "-0.0139\n",
      "-0.0336\n",
      "-0.0297\n",
      "-0.0011\n",
      " 0.0055\n",
      "-0.0180\n",
      "-0.0322\n",
      "-0.0328\n",
      "-0.0242\n",
      " 0.0452\n",
      "-0.0108\n",
      "-0.0011\n",
      "-0.0221\n",
      "-0.0458\n",
      "-0.0733\n",
      "-0.0854\n",
      "-0.0666\n",
      "-0.0764\n",
      "-0.1143\n",
      "-0.0713\n",
      "-0.0819\n",
      "-0.0973\n",
      "-0.0360\n",
      "-0.0972\n",
      "-0.0871\n",
      "-0.0015\n",
      "-0.0325\n",
      " 0.0041\n",
      "-0.0345\n",
      "-0.0203\n",
      "-0.0066\n",
      "-0.0566\n",
      "-0.0695\n",
      "-0.0118\n",
      " 0.0183\n",
      "-0.0482\n",
      "-0.0028\n",
      " 0.0174\n",
      " 0.0077\n",
      " 0.0062\n",
      "-0.0499\n",
      "-0.0899\n",
      "-0.0541\n",
      "-0.0712\n",
      "-0.1004\n",
      "-0.1083\n",
      "-0.0560\n",
      "-0.0735\n",
      "-0.1071\n",
      "-0.0998\n",
      "-0.0717\n",
      "-0.0515\n",
      "-0.0567\n",
      "-0.0393\n",
      "-0.0552\n",
      "-0.0336\n",
      "-0.0687\n",
      "-0.0141\n",
      "-0.0533\n",
      "-0.0148\n",
      "-0.0187\n",
      " 0.0162\n",
      "-0.0222\n",
      "-0.0193\n",
      " 0.0230\n",
      "-0.0069\n",
      "-0.0049\n",
      "-0.0343\n",
      "-0.0536\n",
      "-0.0417\n",
      "-0.0766\n",
      "-0.1239\n",
      "-0.0888\n",
      "-0.0632\n",
      "-0.0614\n",
      "-0.0595\n",
      "-0.0618\n",
      "-0.0507\n",
      "-0.0184\n",
      "-0.0345\n",
      "-0.0626\n",
      " 0.0301\n",
      "-0.0175\n",
      "-0.0057\n",
      "-0.0065\n",
      "-0.0680\n",
      "-0.0246\n",
      "-0.0139\n",
      " 0.0154\n",
      "-0.0207\n",
      "-0.0277\n",
      " 0.0356\n",
      " 0.0171\n",
      " 0.0103\n",
      " 0.0191\n",
      "-0.0493\n",
      "-0.0858\n",
      "-0.0699\n",
      "-0.1114\n",
      "-0.0796\n",
      "-0.0687\n",
      "-0.0706\n",
      "-0.1020\n",
      "-0.1225\n",
      "-0.0578\n",
      "-0.0943\n",
      "-0.0716\n",
      "-0.0721\n",
      "-0.0619\n",
      "-0.0601\n",
      "-0.0296\n",
      "-0.0319\n",
      "-0.0364\n",
      "-0.0235\n",
      "-0.0373\n",
      " 0.0208\n",
      " 0.0130\n",
      "-0.0008\n",
      "-0.0038\n",
      "-0.0121\n",
      "-0.0126\n",
      " 0.0059\n",
      "-0.0559\n",
      "-0.0707\n",
      "-0.1076\n",
      "-0.0671\n",
      "-0.0499\n",
      "-0.0782\n",
      "-0.0756\n",
      "-0.1245\n",
      "-0.0619\n",
      "-0.0917\n",
      "-0.0881\n",
      "-0.0472\n",
      "-0.0715\n",
      "-0.0406\n",
      "-0.0664\n",
      "-0.0155\n",
      " 0.0006\n",
      "-0.0033\n",
      "-0.0185\n",
      " 0.0154\n",
      " 0.0329\n",
      " 0.0228\n",
      " 0.0151\n",
      " 0.0220\n",
      " 0.0234\n",
      "-0.0138\n",
      "-0.0482\n",
      "-0.0390\n",
      "-0.0227\n",
      "-0.0298\n",
      "-0.0658\n",
      "-0.0564\n",
      "-0.0771\n",
      "-0.1086\n",
      "-0.1053\n",
      "-0.1184\n",
      "-0.0784\n",
      "-0.0955\n",
      "-0.1175\n",
      "-0.0999\n",
      "-0.1055\n",
      "-0.0288\n",
      "-0.0704\n",
      "-0.0782\n",
      "-0.0602\n",
      "-0.0298\n",
      " 0.0009\n",
      "-0.0384\n",
      " 0.0111\n",
      "-0.0471\n",
      "-0.0521\n",
      "-0.0108\n",
      "-0.0622\n",
      "-0.0349\n",
      "-0.0192\n",
      "-0.0195\n",
      "-0.0638\n",
      "-0.0468\n",
      "-0.0954\n",
      "-0.0925\n",
      "-0.0909\n",
      "-0.1175\n",
      "-0.0561\n",
      "-0.1068\n",
      "-0.1060\n",
      "-0.0821\n",
      "-0.1233\n",
      "-0.0748\n",
      "-0.0890\n",
      "-0.0638\n",
      "-0.0905\n",
      "-0.0441\n",
      "-0.0142\n",
      "-0.0218\n",
      "-0.0302\n",
      "-0.0783\n",
      "-0.0518\n",
      "-0.0733\n",
      "-0.0923\n",
      "-0.0880\n",
      "-0.0937\n",
      "-0.0471\n",
      "-0.0045\n",
      "-0.0417\n",
      "-0.0492\n",
      "-0.0637\n",
      "-0.0843\n",
      "-0.1304\n",
      "-0.0639\n",
      "-0.1252\n",
      "-0.0902\n",
      "-0.0827\n",
      "-0.1133\n",
      "-0.0473\n",
      "-0.1306\n",
      "-0.0802\n",
      "-0.0833\n",
      "-0.1131\n",
      "-0.0968\n",
      "-0.0669\n",
      "-0.0764\n",
      "-0.0466\n",
      "-0.0599\n",
      "-0.0553\n",
      "-0.1103\n",
      "-0.0629\n",
      "-0.0773\n",
      "-0.0553\n",
      "-0.0458\n",
      "-0.0347\n",
      "-0.0938\n",
      "-0.0491\n",
      "-0.0783\n",
      "-0.0818\n",
      "-0.0545\n",
      "-0.1134\n",
      "-0.0809\n",
      "-0.0816\n",
      "-0.0666\n",
      "-0.1214\n",
      "-0.0651\n",
      "-0.1068\n",
      "-0.0549\n",
      "-0.1140\n",
      "-0.0525\n",
      "-0.0915\n",
      "-0.0505\n",
      "-0.0694\n",
      "-0.0926\n",
      "-0.0541\n",
      "-0.0542\n",
      "-0.0468\n",
      "-0.0732\n",
      "-0.0936\n",
      "-0.1005\n",
      "-0.0891\n",
      "-0.0472\n",
      "-0.0771\n",
      "-0.0904\n",
      "-0.0493\n",
      "-0.0625\n",
      "-0.0926\n",
      "-0.0433\n",
      "-0.0846\n",
      "-0.0754\n",
      "-0.0912\n",
      "-0.0210\n",
      "-0.1261\n",
      "-0.0603\n",
      "-0.1230\n",
      "-0.0905\n",
      "-0.0713\n",
      "-0.0482\n",
      "-0.0827\n",
      "-0.0852\n",
      "-0.0856\n",
      "-0.1090\n",
      "-0.0610\n",
      "-0.0590\n",
      "-0.0885\n",
      "-0.0729\n",
      "-0.0937\n",
      "-0.1194\n",
      "-0.1269\n",
      "-0.1037\n",
      "-0.0999\n",
      "-0.0447\n",
      "-0.0725\n",
      "-0.0804\n",
      "-0.0591\n",
      "-0.0653\n",
      "-0.0954\n",
      "-0.0670\n",
      "-0.1129\n",
      "-0.1047\n",
      "-0.0607\n",
      "-0.0507\n",
      "-0.0897\n",
      "-0.0609\n",
      "-0.0993\n",
      "-0.0565\n",
      "-0.0984\n",
      "-0.1224\n",
      "-0.0998\n",
      "-0.0662\n",
      "-0.0583\n",
      "-0.0855\n",
      "-0.1130\n",
      "-0.0615\n",
      "-0.0822\n",
      "-0.0691\n",
      "-0.0509\n",
      "-0.0989\n",
      "-0.0502\n",
      "-0.0441\n",
      "-0.0836\n",
      "-0.0928\n",
      "-0.0582\n",
      "-0.0773\n",
      "-0.1259\n",
      "-0.0361\n",
      "-0.1210\n",
      "-0.1319\n",
      "-0.1177\n",
      "[torch.cuda.FloatTensor of size 784 (GPU 0)]\n",
      "), ('fc_d2.weight', \n",
      "-2.7558e-02  1.2392e-02 -6.0279e-02  ...   4.9154e-02 -1.0204e-02 -2.4634e-02\n",
      " 6.9643e-02  3.6808e-02 -7.1082e-02  ...   2.7692e-02 -5.8719e-02 -2.5594e-02\n",
      " 5.1139e-02  7.5856e-02 -1.3453e-02  ...   5.0208e-02 -6.3870e-02  9.2055e-03\n",
      "                ...                   ⋱                   ...                \n",
      "-6.7174e-02  7.3766e-02 -1.5588e-03  ...  -6.3993e-02  3.3388e-02 -3.6611e-03\n",
      "-3.2269e-02 -4.0873e-02  8.1667e-03  ...   4.6046e-02  6.3033e-03 -3.5072e-02\n",
      "-3.7748e-02 -8.1573e-03 -4.9233e-02  ...  -3.8175e-02 -2.1560e-02 -8.2807e-02\n",
      "[torch.cuda.FloatTensor of size 500x500 (GPU 0)]\n",
      "), ('fc_d2.bias', \n",
      " 0.0437\n",
      "-0.0200\n",
      " 0.0177\n",
      "-0.0092\n",
      " 0.1123\n",
      " 0.1525\n",
      " 0.1549\n",
      " 0.0454\n",
      "-0.0349\n",
      "-0.0447\n",
      " 0.0221\n",
      " 0.0085\n",
      "-0.0043\n",
      " 0.0338\n",
      "-0.0317\n",
      " 0.0971\n",
      " 0.0170\n",
      "-0.0032\n",
      "-0.0700\n",
      " 0.0081\n",
      " 0.0004\n",
      "-0.0250\n",
      " 0.0282\n",
      "-0.0040\n",
      "-0.0489\n",
      "-0.0399\n",
      " 0.2312\n",
      " 0.0247\n",
      " 0.1208\n",
      " 0.0127\n",
      "-0.0737\n",
      "-0.0045\n",
      " 0.0704\n",
      " 0.0020\n",
      "-0.0268\n",
      " 0.0508\n",
      "-0.0329\n",
      "-0.0315\n",
      "-0.0166\n",
      "-0.0311\n",
      " 0.0220\n",
      " 0.0150\n",
      "-0.0338\n",
      "-0.0557\n",
      " 0.0047\n",
      " 0.0181\n",
      " 0.0892\n",
      " 0.0020\n",
      " 0.0160\n",
      "-0.0070\n",
      " 0.0698\n",
      "-0.0360\n",
      "-0.0148\n",
      "-0.0121\n",
      " 0.0368\n",
      " 0.0010\n",
      " 0.1024\n",
      "-0.0805\n",
      "-0.0306\n",
      "-0.0507\n",
      "-0.0177\n",
      "-0.0472\n",
      " 0.1995\n",
      " 0.0130\n",
      "-0.0006\n",
      "-0.0232\n",
      " 0.0271\n",
      "-0.0108\n",
      "-0.0499\n",
      "-0.1249\n",
      " 0.0558\n",
      "-0.0112\n",
      " 0.0249\n",
      " 0.0852\n",
      "-0.0747\n",
      "-0.0209\n",
      " 0.0553\n",
      "-0.0460\n",
      "-0.0230\n",
      "-0.0268\n",
      "-0.0024\n",
      " 0.0065\n",
      " 0.0131\n",
      "-0.0213\n",
      "-0.0282\n",
      "-0.0191\n",
      " 0.0612\n",
      "-0.0595\n",
      " 0.0335\n",
      " 0.0865\n",
      "-0.0146\n",
      "-0.0922\n",
      "-0.0499\n",
      " 0.0397\n",
      " 0.0835\n",
      "-0.0543\n",
      " 0.0174\n",
      "-0.0221\n",
      "-0.0057\n",
      "-0.0613\n",
      "-0.0085\n",
      " 0.0962\n",
      " 0.0145\n",
      " 0.0010\n",
      "-0.0292\n",
      "-0.0128\n",
      " 0.0264\n",
      " 0.0783\n",
      "-0.0528\n",
      " 0.0579\n",
      " 0.1257\n",
      " 0.0241\n",
      "-0.0873\n",
      " 0.0132\n",
      "-0.0526\n",
      "-0.0516\n",
      "-0.0432\n",
      " 0.0300\n",
      "-0.0566\n",
      "-0.0086\n",
      " 0.2749\n",
      "-0.0209\n",
      "-0.0269\n",
      " 0.0084\n",
      "-0.0238\n",
      "-0.0559\n",
      " 0.0293\n",
      "-0.0107\n",
      " 0.0449\n",
      "-0.0523\n",
      " 0.1699\n",
      " 0.1545\n",
      " 0.0046\n",
      "-0.0265\n",
      " 0.0741\n",
      "-0.0060\n",
      "-0.0326\n",
      " 0.0162\n",
      " 0.0169\n",
      " 0.0407\n",
      "-0.0480\n",
      "-0.0516\n",
      "-0.0146\n",
      "-0.0292\n",
      "-0.0031\n",
      "-0.0348\n",
      "-0.0297\n",
      " 0.0615\n",
      "-0.0288\n",
      "-0.0315\n",
      " 0.0937\n",
      "-0.0456\n",
      "-0.0544\n",
      " 0.0047\n",
      "-0.0189\n",
      "-0.0597\n",
      " 0.0430\n",
      "-0.0331\n",
      " 0.0252\n",
      " 0.1121\n",
      "-0.0332\n",
      "-0.0313\n",
      " 0.0054\n",
      " 0.0044\n",
      "-0.0621\n",
      "-0.0248\n",
      "-0.0122\n",
      "-0.0082\n",
      " 0.0673\n",
      "-0.0415\n",
      " 0.0097\n",
      " 0.0089\n",
      " 0.0336\n",
      "-0.0590\n",
      "-0.0116\n",
      " 0.0014\n",
      "-0.0142\n",
      "-0.0111\n",
      "-0.0072\n",
      " 0.0186\n",
      " 0.0233\n",
      " 0.1591\n",
      "-0.0302\n",
      "-0.0156\n",
      "-0.0317\n",
      " 0.0756\n",
      " 0.2163\n",
      " 0.0464\n",
      " 0.0137\n",
      "-0.0097\n",
      "-0.0502\n",
      "-0.0068\n",
      "-0.0301\n",
      " 0.0982\n",
      "-0.0535\n",
      " 0.0722\n",
      "-0.0623\n",
      " 0.0253\n",
      "-0.0228\n",
      " 0.0296\n",
      "-0.0084\n",
      " 0.0639\n",
      "-0.0770\n",
      "-0.0367\n",
      "-0.0382\n",
      " 0.0498\n",
      " 0.0780\n",
      "-0.0081\n",
      " 0.0245\n",
      "-0.0164\n",
      "-0.0311\n",
      " 0.0263\n",
      "-0.0465\n",
      "-0.0213\n",
      " 0.0579\n",
      "-0.0683\n",
      " 0.0281\n",
      "-0.0628\n",
      "-0.0030\n",
      "-0.0519\n",
      " 0.0704\n",
      " 0.0058\n",
      "-0.0002\n",
      "-0.0406\n",
      "-0.0290\n",
      " 0.0016\n",
      " 0.0189\n",
      "-0.0006\n",
      "-0.0791\n",
      "-0.0004\n",
      " 0.0761\n",
      "-0.0215\n",
      "-0.0653\n",
      "-0.0710\n",
      "-0.0242\n",
      " 0.0222\n",
      " 0.0189\n",
      " 0.1355\n",
      "-0.0326\n",
      "-0.0575\n",
      "-0.0337\n",
      "-0.0600\n",
      "-0.0494\n",
      " 0.1488\n",
      "-0.0058\n",
      "-0.0137\n",
      " 0.2271\n",
      " 0.2321\n",
      "-0.0337\n",
      "-0.0001\n",
      "-0.0433\n",
      " 0.0521\n",
      "-0.0129\n",
      "-0.0352\n",
      " 0.0189\n",
      " 0.1274\n",
      " 0.0158\n",
      "-0.0417\n",
      " 0.0318\n",
      "-0.0102\n",
      " 0.0079\n",
      " 0.0328\n",
      " 0.0046\n",
      " 0.0166\n",
      " 0.0045\n",
      " 0.1868\n",
      " 0.0025\n",
      "-0.0384\n",
      " 0.0023\n",
      " 0.0205\n",
      " 0.0685\n",
      " 0.1472\n",
      "-0.0276\n",
      " 0.1043\n",
      "-0.0246\n",
      "-0.0117\n",
      "-0.0442\n",
      " 0.0630\n",
      " 0.0749\n",
      "-0.0199\n",
      " 0.0103\n",
      " 0.1126\n",
      "-0.0283\n",
      " 0.0706\n",
      " 0.0547\n",
      "-0.0195\n",
      "-0.0500\n",
      "-0.0249\n",
      " 0.0206\n",
      "-0.0119\n",
      "-0.0469\n",
      " 0.1241\n",
      " 0.0084\n",
      " 0.0669\n",
      "-0.0132\n",
      " 0.0247\n",
      " 0.0129\n",
      "-0.0608\n",
      " 0.0326\n",
      "-0.0162\n",
      " 0.1427\n",
      " 0.0043\n",
      "-0.0311\n",
      " 0.0747\n",
      "-0.0449\n",
      "-0.0013\n",
      " 0.0020\n",
      " 0.1412\n",
      "-0.0015\n",
      "-0.0362\n",
      " 0.0407\n",
      "-0.0481\n",
      " 0.0108\n",
      " 0.0282\n",
      " 0.0072\n",
      " 0.0354\n",
      " 0.0121\n",
      " 0.0239\n",
      "-0.0784\n",
      " 0.0405\n",
      "-0.0194\n",
      "-0.0026\n",
      " 0.1707\n",
      "-0.0385\n",
      "-0.0003\n",
      " 0.0402\n",
      " 0.1469\n",
      " 0.1308\n",
      " 0.0029\n",
      " 0.1352\n",
      " 0.0377\n",
      "-0.0240\n",
      "-0.0169\n",
      "-0.0188\n",
      "-0.0052\n",
      "-0.0600\n",
      " 0.0124\n",
      "-0.0258\n",
      " 0.0342\n",
      "-0.0301\n",
      "-0.0199\n",
      "-0.0187\n",
      " 0.0174\n",
      " 0.0055\n",
      " 0.0051\n",
      "-0.0377\n",
      " 0.0039\n",
      "-0.0131\n",
      " 0.0168\n",
      " 0.0278\n",
      " 0.0006\n",
      "-0.0508\n",
      " 0.0185\n",
      "-0.0408\n",
      "-0.0132\n",
      "-0.0053\n",
      "-0.0422\n",
      " 0.0254\n",
      " 0.0388\n",
      " 0.0322\n",
      "-0.0883\n",
      "-0.0305\n",
      "-0.0177\n",
      "-0.0681\n",
      " 0.0619\n",
      " 0.2146\n",
      "-0.0238\n",
      " 0.0221\n",
      "-0.0225\n",
      "-0.0130\n",
      "-0.0465\n",
      " 0.0307\n",
      "-0.0344\n",
      " 0.0020\n",
      "-0.0157\n",
      "-0.0182\n",
      "-0.0620\n",
      " 0.0130\n",
      "-0.0049\n",
      " 0.1717\n",
      " 0.0580\n",
      " 0.0332\n",
      "-0.1009\n",
      "-0.0442\n",
      "-0.0027\n",
      " 0.0821\n",
      " 0.0741\n",
      "-0.0429\n",
      "-0.0428\n",
      " 0.0125\n",
      " 0.1136\n",
      " 0.0529\n",
      "-0.0129\n",
      " 0.0941\n",
      "-0.0499\n",
      " 0.1027\n",
      " 0.0511\n",
      " 0.0134\n",
      "-0.0101\n",
      "-0.0118\n",
      "-0.0340\n",
      " 0.1225\n",
      "-0.0079\n",
      "-0.0004\n",
      "-0.0248\n",
      "-0.0099\n",
      "-0.0172\n",
      " 0.0698\n",
      " 0.1596\n",
      "-0.0263\n",
      "-0.0041\n",
      "-0.0275\n",
      " 0.0107\n",
      " 0.0262\n",
      " 0.1952\n",
      "-0.0305\n",
      " 0.0165\n",
      " 0.0417\n",
      " 0.0050\n",
      "-0.0025\n",
      "-0.0189\n",
      "-0.0361\n",
      " 0.0446\n",
      " 0.0072\n",
      "-0.0420\n",
      "-0.0517\n",
      "-0.0017\n",
      " 0.0999\n",
      "-0.0365\n",
      " 0.0893\n",
      "-0.0314\n",
      " 0.0544\n",
      "-0.0664\n",
      " 0.0173\n",
      "-0.0257\n",
      "-0.0456\n",
      "-0.0453\n",
      " 0.1025\n",
      "-0.0602\n",
      " 0.0261\n",
      "-0.0421\n",
      " 0.1321\n",
      "-0.0089\n",
      "-0.0494\n",
      " 0.0041\n",
      " 0.0833\n",
      "-0.0867\n",
      "-0.0618\n",
      "-0.0012\n",
      " 0.0265\n",
      "-0.0458\n",
      " 0.0954\n",
      " 0.0560\n",
      "-0.0304\n",
      "-0.0399\n",
      "-0.0623\n",
      " 0.0465\n",
      "-0.0183\n",
      "-0.0487\n",
      " 0.1363\n",
      " 0.0061\n",
      " 0.0396\n",
      " 0.0186\n",
      "-0.0053\n",
      " 0.0055\n",
      " 0.0068\n",
      "-0.0048\n",
      "-0.0380\n",
      " 0.0130\n",
      " 0.0950\n",
      " 0.1122\n",
      " 0.0057\n",
      " 0.0438\n",
      "-0.0401\n",
      " 0.0356\n",
      " 0.0271\n",
      " 0.0254\n",
      " 0.1809\n",
      "-0.0254\n",
      "-0.0419\n",
      "-0.0381\n",
      "-0.0096\n",
      " 0.0886\n",
      "-0.0175\n",
      "-0.0216\n",
      "-0.0257\n",
      "-0.0241\n",
      " 0.0321\n",
      " 0.0152\n",
      "-0.0156\n",
      "-0.0001\n",
      "-0.0370\n",
      "-0.0335\n",
      " 0.0836\n",
      "-0.0355\n",
      "-0.0193\n",
      "-0.0208\n",
      " 0.1446\n",
      " 0.0255\n",
      " 0.0596\n",
      "[torch.cuda.FloatTensor of size 500 (GPU 0)]\n",
      "), ('fc_d3.weight', \n",
      " 8.0108e-04  3.1661e-03  4.3509e-02  ...  -2.1762e-02 -5.0507e-03 -3.7813e-03\n",
      " 4.2339e-02  2.8244e-02 -3.3384e-02  ...  -2.7605e-02 -1.4365e-02 -1.3276e-02\n",
      " 3.1940e-02  3.8141e-02  2.1663e-02  ...   1.0761e-02  3.6730e-02 -3.4015e-02\n",
      "                ...                   ⋱                   ...                \n",
      "-6.8776e-03 -3.3828e-02 -3.2391e-02  ...  -2.0586e-02  2.1353e-03  3.6944e-03\n",
      "-2.6813e-02 -5.7095e-03 -2.1980e-02  ...   7.2314e-03 -4.4138e-02 -7.1430e-03\n",
      " 7.8046e-03 -4.1328e-02  4.1846e-02  ...  -1.4027e-02  2.3358e-02 -2.1976e-02\n",
      "[torch.cuda.FloatTensor of size 500x2000 (GPU 0)]\n",
      "), ('fc_d3.bias', \n",
      "-0.0160\n",
      "-0.0091\n",
      "-0.0243\n",
      "-0.0228\n",
      "-0.0223\n",
      " 0.0147\n",
      "-0.0502\n",
      "-0.0170\n",
      "-0.0029\n",
      " 0.0227\n",
      "-0.0074\n",
      " 0.0127\n",
      "-0.0086\n",
      " 0.0426\n",
      " 0.0041\n",
      "-0.0090\n",
      "-0.0148\n",
      "-0.0155\n",
      " 0.0131\n",
      "-0.0024\n",
      " 0.0008\n",
      " 0.0042\n",
      "-0.0147\n",
      "-0.0084\n",
      "-0.0234\n",
      " 0.0426\n",
      " 0.0042\n",
      " 0.0268\n",
      "-0.0017\n",
      " 0.0106\n",
      "-0.0063\n",
      " 0.1176\n",
      "-0.0098\n",
      " 0.0521\n",
      "-0.0012\n",
      "-0.0036\n",
      " 0.0406\n",
      " 0.0096\n",
      " 0.0347\n",
      " 0.0025\n",
      " 0.0148\n",
      "-0.0122\n",
      " 0.0087\n",
      " 0.0126\n",
      "-0.0054\n",
      " 0.0170\n",
      " 0.0122\n",
      "-0.0038\n",
      "-0.0027\n",
      "-0.0355\n",
      "-0.0028\n",
      " 0.0063\n",
      " 0.0158\n",
      " 0.0517\n",
      "-0.0166\n",
      "-0.0254\n",
      "-0.0198\n",
      "-0.0057\n",
      "-0.0135\n",
      " 0.0263\n",
      " 0.0157\n",
      " 0.0095\n",
      " 0.0159\n",
      "-0.0366\n",
      " 0.0029\n",
      "-0.0193\n",
      " 0.0537\n",
      " 0.0521\n",
      "-0.0262\n",
      " 0.0208\n",
      " 0.0184\n",
      "-0.0051\n",
      "-0.0117\n",
      " 0.0116\n",
      "-0.0110\n",
      " 0.0049\n",
      "-0.0281\n",
      "-0.0065\n",
      " 0.0237\n",
      "-0.0390\n",
      "-0.0107\n",
      " 0.0052\n",
      "-0.0155\n",
      " 0.0018\n",
      "-0.0381\n",
      " 0.0290\n",
      " 0.0044\n",
      "-0.0225\n",
      " 0.0081\n",
      "-0.0217\n",
      " 0.0040\n",
      "-0.0437\n",
      "-0.0201\n",
      " 0.0028\n",
      "-0.0100\n",
      " 0.0259\n",
      "-0.0025\n",
      " 0.0315\n",
      "-0.0329\n",
      "-0.0160\n",
      " 0.0128\n",
      "-0.0171\n",
      " 0.0241\n",
      "-0.0322\n",
      "-0.0381\n",
      "-0.0118\n",
      "-0.0001\n",
      " 0.0188\n",
      "-0.0108\n",
      "-0.0248\n",
      " 0.0133\n",
      "-0.0095\n",
      " 0.0037\n",
      " 0.0100\n",
      "-0.0135\n",
      " 0.0120\n",
      "-0.0195\n",
      "-0.0109\n",
      "-0.0212\n",
      "-0.0118\n",
      " 0.0209\n",
      " 0.0280\n",
      "-0.0229\n",
      " 0.0223\n",
      "-0.0044\n",
      "-0.0028\n",
      " 0.0079\n",
      " 0.0153\n",
      "-0.0365\n",
      "-0.0022\n",
      " 0.0073\n",
      "-0.0256\n",
      "-0.0148\n",
      " 0.0039\n",
      " 0.0275\n",
      " 0.0169\n",
      "-0.0237\n",
      " 0.0020\n",
      "-0.0082\n",
      " 0.0511\n",
      "-0.0042\n",
      "-0.0428\n",
      " 0.0082\n",
      "-0.0130\n",
      "-0.0235\n",
      "-0.0210\n",
      " 0.0348\n",
      " 0.0375\n",
      "-0.0148\n",
      "-0.0041\n",
      " 0.0140\n",
      " 0.0067\n",
      " 0.0084\n",
      " 0.0146\n",
      " 0.0461\n",
      "-0.0251\n",
      " 0.0122\n",
      " 0.0231\n",
      "-0.0060\n",
      "-0.0065\n",
      " 0.0203\n",
      "-0.0159\n",
      " 0.0019\n",
      " 0.0516\n",
      "-0.0262\n",
      "-0.0114\n",
      " 0.0384\n",
      " 0.0236\n",
      " 0.0093\n",
      "-0.0185\n",
      "-0.0223\n",
      " 0.0064\n",
      " 0.0388\n",
      "-0.0383\n",
      "-0.0042\n",
      " 0.0033\n",
      " 0.0225\n",
      "-0.0049\n",
      " 0.0107\n",
      " 0.0240\n",
      "-0.0114\n",
      " 0.0474\n",
      " 0.0423\n",
      "-0.0088\n",
      " 0.0343\n",
      "-0.0187\n",
      " 0.0351\n",
      " 0.0170\n",
      " 0.0177\n",
      "-0.0277\n",
      " 0.0588\n",
      "-0.0481\n",
      "-0.0302\n",
      "-0.0195\n",
      " 0.0044\n",
      " 0.0264\n",
      " 0.0109\n",
      " 0.0005\n",
      " 0.0107\n",
      " 0.0183\n",
      " 0.0317\n",
      " 0.0079\n",
      " 0.0118\n",
      "-0.0030\n",
      " 0.0119\n",
      "-0.0260\n",
      " 0.0081\n",
      "-0.0029\n",
      "-0.0364\n",
      "-0.0314\n",
      "-0.0161\n",
      " 0.0302\n",
      "-0.0138\n",
      "-0.0266\n",
      "-0.0282\n",
      "-0.0167\n",
      " 0.0019\n",
      "-0.0319\n",
      "-0.0236\n",
      " 0.0490\n",
      " 0.0068\n",
      " 0.0106\n",
      " 0.0337\n",
      "-0.0245\n",
      " 0.0233\n",
      " 0.0052\n",
      "-0.0398\n",
      "-0.0183\n",
      " 0.0158\n",
      " 0.0137\n",
      "-0.0167\n",
      " 0.0273\n",
      " 0.0016\n",
      " 0.0009\n",
      " 0.0144\n",
      " 0.0167\n",
      "-0.0338\n",
      " 0.0068\n",
      "-0.0185\n",
      " 0.0028\n",
      " 0.0220\n",
      " 0.0136\n",
      " 0.0064\n",
      "-0.0208\n",
      " 0.0091\n",
      "-0.0073\n",
      "-0.0213\n",
      "-0.0118\n",
      " 0.0011\n",
      "-0.0212\n",
      "-0.0381\n",
      " 0.0016\n",
      " 0.0117\n",
      "-0.0249\n",
      "-0.0140\n",
      " 0.0015\n",
      "-0.0022\n",
      " 0.0505\n",
      "-0.0481\n",
      "-0.0064\n",
      " 0.0096\n",
      "-0.0068\n",
      " 0.0131\n",
      "-0.0027\n",
      " 0.0060\n",
      " 0.0231\n",
      " 0.0235\n",
      "-0.0368\n",
      "-0.0196\n",
      "-0.0176\n",
      " 0.0129\n",
      "-0.0008\n",
      " 0.0418\n",
      " 0.0010\n",
      "-0.0119\n",
      " 0.0249\n",
      " 0.0046\n",
      "-0.0317\n",
      " 0.0467\n",
      "-0.0031\n",
      " 0.0212\n",
      "-0.0054\n",
      "-0.0056\n",
      "-0.0003\n",
      "-0.0313\n",
      "-0.0217\n",
      "-0.0136\n",
      "-0.0121\n",
      " 0.0281\n",
      "-0.0097\n",
      "-0.0068\n",
      " 0.0203\n",
      "-0.0077\n",
      "-0.0029\n",
      " 0.0113\n",
      "-0.0046\n",
      " 0.0037\n",
      "-0.0179\n",
      " 0.0002\n",
      "-0.0262\n",
      " 0.0076\n",
      "-0.0188\n",
      "-0.0230\n",
      " 0.0272\n",
      " 0.0295\n",
      " 0.0423\n",
      "-0.0318\n",
      "-0.0009\n",
      "-0.0188\n",
      "-0.0105\n",
      " 0.0238\n",
      " 0.0136\n",
      " 0.0094\n",
      " 0.0236\n",
      " 0.0290\n",
      "-0.0094\n",
      "-0.0209\n",
      "-0.0100\n",
      "-0.0186\n",
      " 0.0178\n",
      "-0.0001\n",
      " 0.0063\n",
      "-0.0178\n",
      "-0.0164\n",
      " 0.0263\n",
      "-0.0005\n",
      " 0.0506\n",
      "-0.0022\n",
      " 0.0051\n",
      "-0.0088\n",
      " 0.0206\n",
      "-0.0280\n",
      "-0.0263\n",
      " 0.0182\n",
      "-0.0014\n",
      " 0.0279\n",
      " 0.0145\n",
      "-0.0087\n",
      " 0.0224\n",
      "-0.0292\n",
      " 0.0006\n",
      "-0.0414\n",
      " 0.0063\n",
      " 0.0020\n",
      " 0.0609\n",
      "-0.0164\n",
      "-0.0127\n",
      "-0.0161\n",
      "-0.0281\n",
      " 0.0363\n",
      " 0.0155\n",
      " 0.0096\n",
      " 0.0638\n",
      " 0.0096\n",
      "-0.0093\n",
      "-0.0299\n",
      "-0.0095\n",
      "-0.0360\n",
      "-0.0202\n",
      " 0.0323\n",
      "-0.0412\n",
      "-0.0004\n",
      "-0.0158\n",
      " 0.0005\n",
      "-0.0264\n",
      "-0.0170\n",
      " 0.0416\n",
      " 0.0046\n",
      " 0.0202\n",
      " 0.0484\n",
      "-0.0325\n",
      "-0.0277\n",
      " 0.0038\n",
      " 0.0130\n",
      "-0.0062\n",
      " 0.0400\n",
      " 0.0306\n",
      " 0.0156\n",
      " 0.0330\n",
      " 0.0206\n",
      " 0.0031\n",
      "-0.0339\n",
      "-0.0049\n",
      " 0.0198\n",
      " 0.0188\n",
      " 0.0307\n",
      " 0.0214\n",
      " 0.0042\n",
      " 0.0151\n",
      "-0.0108\n",
      " 0.0029\n",
      " 0.0159\n",
      " 0.0471\n",
      "-0.0210\n",
      " 0.0136\n",
      "-0.0184\n",
      " 0.0121\n",
      " 0.0256\n",
      " 0.0064\n",
      " 0.0086\n",
      "-0.0383\n",
      " 0.0187\n",
      " 0.0313\n",
      "-0.0047\n",
      "-0.0331\n",
      "-0.0305\n",
      " 0.0419\n",
      " 0.0018\n",
      " 0.0178\n",
      "-0.0259\n",
      "-0.0329\n",
      "-0.0026\n",
      "-0.0022\n",
      " 0.0169\n",
      " 0.0115\n",
      " 0.0228\n",
      "-0.0136\n",
      "-0.0113\n",
      " 0.0642\n",
      "-0.0266\n",
      "-0.0221\n",
      "-0.0238\n",
      "-0.0024\n",
      "-0.0348\n",
      "-0.0081\n",
      " 0.0212\n",
      " 0.0137\n",
      "-0.0222\n",
      "-0.0012\n",
      "-0.0126\n",
      "-0.0278\n",
      " 0.0456\n",
      "-0.0324\n",
      "-0.0271\n",
      " 0.0137\n",
      " 0.0337\n",
      "-0.0074\n",
      " 0.0073\n",
      "-0.0319\n",
      " 0.0041\n",
      "-0.0092\n",
      " 0.0022\n",
      "-0.0057\n",
      " 0.0217\n",
      " 0.0286\n",
      "-0.0057\n",
      "-0.0395\n",
      "-0.0269\n",
      " 0.0058\n",
      " 0.0312\n",
      "-0.0097\n",
      " 0.0819\n",
      "-0.0031\n",
      "-0.0090\n",
      "-0.0255\n",
      "-0.0147\n",
      " 0.0280\n",
      "-0.0119\n",
      " 0.0002\n",
      " 0.0176\n",
      "-0.0002\n",
      " 0.0012\n",
      " 0.0051\n",
      " 0.0272\n",
      "-0.0071\n",
      " 0.0034\n",
      " 0.0160\n",
      "-0.0073\n",
      "-0.0193\n",
      "-0.0340\n",
      "-0.0114\n",
      "-0.0327\n",
      " 0.0349\n",
      "-0.0297\n",
      "-0.0037\n",
      "-0.0445\n",
      "-0.0236\n",
      "-0.0090\n",
      " 0.0532\n",
      " 0.0374\n",
      " 0.0102\n",
      " 0.0231\n",
      "-0.0156\n",
      " 0.0412\n",
      "-0.0021\n",
      " 0.0003\n",
      " 0.0389\n",
      "-0.0202\n",
      "-0.0235\n",
      " 0.0024\n",
      "-0.0044\n",
      "-0.0168\n",
      "-0.0205\n",
      "-0.0093\n",
      " 0.0393\n",
      " 0.0368\n",
      " 0.0380\n",
      " 0.0025\n",
      "-0.0024\n",
      "-0.0002\n",
      "[torch.cuda.FloatTensor of size 500 (GPU 0)]\n",
      "), ('fc_d4.weight', \n",
      "-1.9321e-02 -3.3287e-02  8.8118e-02  ...   2.8659e-04 -3.3224e-02 -5.3777e-02\n",
      "-5.1945e-02  5.1119e-02 -5.7062e-02  ...   6.2040e-02 -1.5229e-02  1.0538e-01\n",
      "-2.9051e-02 -6.1763e-02  3.2109e-02  ...   1.8084e-02  1.2035e-02  5.1234e-02\n",
      "                ...                   ⋱                   ...                \n",
      "-2.7286e-02 -4.6343e-03 -7.4429e-03  ...  -4.5858e-02  4.0898e-02 -1.1281e-02\n",
      "-1.5925e-02 -7.1415e-02 -2.5828e-02  ...   6.1433e-03  9.3108e-03  3.8159e-02\n",
      "-4.0814e-03  4.3149e-02 -5.8562e-03  ...  -1.0489e-01 -1.1236e-02 -1.4917e-02\n",
      "[torch.cuda.FloatTensor of size 2000x10 (GPU 0)]\n",
      "), ('fc_d4.bias', \n",
      " 0.1378\n",
      "-0.2544\n",
      " 0.0671\n",
      "   ⋮   \n",
      "-0.2997\n",
      "-0.2395\n",
      "-0.1049\n",
      "[torch.cuda.FloatTensor of size 2000 (GPU 0)]\n",
      ")])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type Variable[torch.cuda.FloatTensor] but found type Variable[torch.FloatTensor] for argument #1 'other'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2208c7a639b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#dec.pretrain(train_loader, test_loader, 10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-c08a08bcbe85>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, test_loader, epochs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0;31m#now we start training with acquired cluster center\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                     \u001b[0mfeature_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                     \u001b[0;31m#output (batchSize,n_cluster)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetTDistribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-d26117af5dee>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m#if not in pretrain mode, we only need encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrainMode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetTDistribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# 1x68\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-d26117af5dee>\u001b[0m in \u001b[0;36mgetTDistribution\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n\u001b[1;32m     31\u001b[0m          q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\"\"\"\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mxe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclusterCenter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of type Variable[torch.cuda.FloatTensor] but found type Variable[torch.FloatTensor] for argument #1 'other'"
     ]
    }
   ],
   "source": [
    "#now start training\n",
    "import random\n",
    "random.seed(1234)\n",
    "dec = DEC(10)\n",
    "#dec.pretrain(train_loader, test_loader, 10)\n",
    "dec.train(train_loader, test_loader, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
