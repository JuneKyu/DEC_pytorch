{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "## load mnist dataset\n",
    "use_cuda = torch.cuda.is_available()\n",
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "#trans = transforms.Compose([transforms.Resize(32), transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "# if not exist, download mnist dataset\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)\n",
    "batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timingResult = {}\n",
    "def logTime(theName, currentTime):\n",
    "    if theName not in timingResult:\n",
    "        timingResult[theName] = time.time() - currentTime\n",
    "    else:\n",
    "        timingResult[theName] = timingResult[theName] + (time.time() - currentTime)\n",
    "    currentTime = time.time()\n",
    "    return currentTime\n",
    "\n",
    "def printTiming(name):\n",
    "    print('======== timing for {}: {} ======='.format(name,timingResult[name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEC_AE(nn.Module):\n",
    "    def __init__(self, num_classes, num_features):\n",
    "        super(DEC_AE,self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.fc1 = nn.Linear(28*28,500)\n",
    "        self.fc2 = nn.Linear(500,500)\n",
    "        self.fc3 = nn.Linear(500,2000)\n",
    "        self.fc4 = nn.Linear(2000,num_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_d1 = nn.Linear(500,28*28)\n",
    "        self.fc_d2 = nn.Linear(500,500)\n",
    "        self.fc_d3 = nn.Linear(2000,500)\n",
    "        self.fc_d4 = nn.Linear(num_features,2000)\n",
    "        self.alpha = 1.0\n",
    "        self.clusterCenter = nn.Parameter(torch.zeros(num_classes,num_features))\n",
    "        self.pretrainMode = True\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform(m.weight)\n",
    "\n",
    "    def setPretrain(self,mode):\n",
    "        \"\"\"To set training mode to pretrain or not, \n",
    "        so that it can control to run only the AE or AE+DECODER\"\"\"\n",
    "        self.pretrainMode = mode\n",
    "    \n",
    "    def updateClusterCenter(self, cc):\n",
    "        self.clusterCenter.data = torch.from_numpy(cc)\n",
    "        \n",
    "    def getTDistribution(self,x, clusterCenter):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "         q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\"\"\"\n",
    "        xe = torch.unsqueeze(x,1).cuda() - clusterCenter.cuda()\n",
    "        q = 1.0 / (1.0 + (torch.sum(torch.mul(xe,xe), 2) / self.alpha))\n",
    "        q = q ** (self.alpha + 1.0) / 2.0\n",
    "        q = (q.t() / torch.sum(q, 1)).t() #due to divison, we need to transpose q\n",
    "        return q\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(-1, 1*28*28)\n",
    "        # 32x32x1\n",
    "        x = self.dropout(x)\n",
    "        # 32x32x1\n",
    "        x = self.fc1(x)\n",
    "        # 17x17x50\n",
    "        x = self.relu(x)\n",
    "        # 17x17x50\n",
    "        x = self.fc2(x)\n",
    "        # 17x17x50\n",
    "        x = self.relu(x)\n",
    "        # 9x9x50\n",
    "        x = self.fc3(x)\n",
    "        # 17x17x50\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        # 9x9x50\n",
    "        x_ae = x\n",
    "        #if not in pretrain mode, we only need encoder\n",
    "        if self.pretrainMode == False:\n",
    "            return x, self.getTDistribution(x,self.clusterCenter)\n",
    "        # 1x68\n",
    "        ##### auto encoder is done, followed by decoder #####\n",
    "        # 1x68\n",
    "        x = self.fc_d4(x)\n",
    "        # 1x4050\n",
    "        x = self.relu(x)\n",
    "        # 1x4050\n",
    "        x = self.fc_d3(x)\n",
    "        # 1x4050\n",
    "        x = self.relu(x)\n",
    "        x = self.fc_d2(x)\n",
    "        # 1x4050\n",
    "        x = self.relu(x)\n",
    "        x = self.fc_d1(x)\n",
    "        x_de = x.view(-1,1,28,28)\n",
    "        # 1x4050\n",
    "        return x_ae, x_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "nmi = normalized_mutual_info_score\n",
    "ari = adjusted_rand_score\n",
    "\n",
    "\n",
    "def acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    \n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEC:\n",
    "    \"\"\"The class for controlling the training process of DEC\"\"\"\n",
    "    def __init__(self,n_clusters,alpha=1.0):\n",
    "        self.n_clusters=n_clusters\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    @staticmethod\n",
    "    def target_distribution(q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        #print('q',q)\n",
    "        return Variable((weight.t() / weight.sum(1)).t().data, requires_grad=True)\n",
    "    def logAccuracy(self,pred,label):\n",
    "        print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
    "          % (acc(label, pred), nmi(label, pred)))\n",
    "    def kld(self,q,p):\n",
    "        res = torch.sum(p*torch.log(p/q),dim=-1)\n",
    "        #print('kld ==> ', res)\n",
    "        return res\n",
    "    \n",
    "    def validateOnCompleteTestData(self,test_loader,model):\n",
    "        model.eval()\n",
    "        to_eval = np.array([model(d[0].cuda())[0].data.cpu().numpy() for i,d in enumerate(test_loader)])\n",
    "        true_labels = np.array([d[1].cpu().numpy() for i,d in enumerate(test_loader)])\n",
    "        to_eval = np.reshape(to_eval,(to_eval.shape[0]*to_eval.shape[1],to_eval.shape[2]))\n",
    "        print(to_eval.shape)\n",
    "        true_labels = np.reshape(true_labels,true_labels.shape[0]*true_labels.shape[1])\n",
    "        km = KMeans(n_clusters=len(np.unique(true_labels)), n_init=20, n_jobs=4)\n",
    "        y_pred = km.fit_predict(to_eval)\n",
    "        print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
    "                      % (acc(true_labels, y_pred), nmi(true_labels, y_pred)))\n",
    "        currentAcc = acc(true_labels, y_pred)\n",
    "        return currentAcc\n",
    "    \n",
    "    def pretrain(self,train_loader, test_loader, epochs):        \n",
    "        dec_ae = DEC_AE(10,10).cuda() #auto encoder\n",
    "        mseloss = nn.MSELoss()\n",
    "        #rint([i for i in dec_ae.parameters()])\n",
    "        optimizer = optim.SGD(dec_ae.parameters(),lr = 1, momentum=0.9)\n",
    "        best_acc = 0.0\n",
    "        for epoch in range(epochs):\n",
    "            dec_ae.train()\n",
    "            running_loss=0.0\n",
    "            to_eval = []\n",
    "            true_labels = []\n",
    "            for i,data in enumerate(train_loader):\n",
    "                x, label = data\n",
    "                x,label=Variable(x).cuda(),Variable(label).cuda()\n",
    "                optimizer.zero_grad()\n",
    "                x_ae,x_de = dec_ae(x)\n",
    "                loss = F.mse_loss(x_de,x,reduce=True) \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                x_eval = x.data.cpu().numpy()\n",
    "                label_eval = label.data.cpu().numpy()\n",
    "                # print statistics\n",
    "                running_loss += loss.data.cpu().numpy()[0]\n",
    "                if i % 100 == 99:    # print every 2000 mini-batches\n",
    "                    print('[%d, %5d] loss: %.7f' %\n",
    "                          (epoch + 1, i + 1, running_loss / 100))\n",
    "                    #print('x_de:',x_de, x)\n",
    "                    running_loss = 0.0\n",
    "            #now we evaluate the accuracy with AE\n",
    "            dec_ae.eval()\n",
    "            currentAcc = self.validateOnCompleteTestData(test_loader,dec_ae)\n",
    "            if currentAcc > best_acc:                \n",
    "                torch.save(dec_ae,'bestModel'.format(best_acc))\n",
    "                best_acc = currentAcc\n",
    "                \n",
    "\n",
    "    \n",
    "    def train(self,train_loader, test_loader, epochs):\n",
    "        \"\"\"This method will start training for DEC cluster\"\"\"\n",
    "        ct = time.time()\n",
    "        model = torch.load(\"bestModel\").cuda()\n",
    "        model.setPretrain(False)\n",
    "        optimizer = optim.SGD([\\\n",
    "             {'params': model.parameters()}, \\\n",
    "            ],lr = 0.001, momentum=0.9)\n",
    "        print('Initializing cluster center with pre-trained weights')\n",
    "        km = MiniBatchKMeans(n_clusters=self.n_clusters, n_init=20, batch_size=batch_size)\n",
    "        got_cluster_center = False\n",
    "        for epoch in range(epochs):\n",
    "            running_loss=0.0\n",
    "            for i,data in enumerate(train_loader):\n",
    "                x, label = data\n",
    "                x = Variable(x).cuda()\n",
    "                optimizer.zero_grad()\n",
    "                #step 1 - get cluster center from batch\n",
    "                if not got_cluster_center:                \n",
    "                    model.eval()\n",
    "                    y_pred_ae,_ = model(x)\n",
    "                    #print('y_pred',y_pred_ae)                    \n",
    "                    y_pred_ae = y_pred_ae.data.cpu().numpy()\n",
    "                    #print('ae prediction', y_pred_ae.shape)\n",
    "                    y_pred = km.partial_fit(y_pred_ae) #seems we can only get a centre from batch\n",
    "                    #print('cluster center:',km.cluster_centers_.shape)\n",
    "                    self.cluster_centers = km.cluster_centers_ #keep the cluster centers\n",
    "                    model.updateClusterCenter(self.cluster_centers)\n",
    "                    #rint('model',model.state_dict())\n",
    "                    if epoch > 1:\n",
    "                        got_cluster_center = True\n",
    "                else:\n",
    "                    model.train()\n",
    "                    #now we start training with acquired cluster center\n",
    "                    feature_pred,q= model(x)\n",
    "                    #get target distribution\n",
    "                    p = self.target_distribution(q)\n",
    "                    #print('q',q,'p',p)\n",
    "                    loss = self.kld(q,p).mean()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss = running_loss + loss.data.cpu().numpy()[0]\n",
    "            currentAcc = self.validateOnCompleteTestData(test_loader,model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.0779668\n",
      "[1,   200] loss: 0.0595135\n",
      "[1,   300] loss: 0.0483055\n",
      "[1,   400] loss: 0.0406491\n",
      "[1,   500] loss: 0.0361709\n",
      "[1,   600] loss: 0.0338063\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.5194,  nmi: 0.4881  <==|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/private/xlia/anaconda3/envs/py35/lib/python3.5/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type DEC_AE. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,   100] loss: 0.0317056\n",
      "[2,   200] loss: 0.0307140\n",
      "[2,   300] loss: 0.0291085\n",
      "[2,   400] loss: 0.0282583\n",
      "[2,   500] loss: 0.0274381\n",
      "[2,   600] loss: 0.0270275\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.5581,  nmi: 0.5278  <==|\n",
      "[3,   100] loss: 0.0261021\n",
      "[3,   200] loss: 0.0258156\n",
      "[3,   300] loss: 0.0253449\n",
      "[3,   400] loss: 0.0247645\n",
      "[3,   500] loss: 0.0244093\n",
      "[3,   600] loss: 0.0242366\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.5831,  nmi: 0.5517  <==|\n",
      "[4,   100] loss: 0.0237356\n",
      "[4,   200] loss: 0.0234857\n",
      "[4,   300] loss: 0.0232656\n",
      "[4,   400] loss: 0.0231485\n",
      "[4,   500] loss: 0.0225917\n",
      "[4,   600] loss: 0.0225319\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.5944,  nmi: 0.5638  <==|\n",
      "[5,   100] loss: 0.0223180\n",
      "[5,   200] loss: 0.0219938\n",
      "[5,   300] loss: 0.0217699\n",
      "[5,   400] loss: 0.0216775\n",
      "[5,   500] loss: 0.0212407\n",
      "[5,   600] loss: 0.0213630\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6206,  nmi: 0.5839  <==|\n",
      "[6,   100] loss: 0.0210852\n",
      "[6,   200] loss: 0.0209596\n",
      "[6,   300] loss: 0.0206662\n",
      "[6,   400] loss: 0.0206058\n",
      "[6,   500] loss: 0.0205684\n",
      "[6,   600] loss: 0.0205042\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6504,  nmi: 0.6085  <==|\n",
      "[7,   100] loss: 0.0203383\n",
      "[7,   200] loss: 0.0200877\n",
      "[7,   300] loss: 0.0198908\n",
      "[7,   400] loss: 0.0198076\n",
      "[7,   500] loss: 0.0196840\n",
      "[7,   600] loss: 0.0197404\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6575,  nmi: 0.6125  <==|\n",
      "[8,   100] loss: 0.0195350\n",
      "[8,   200] loss: 0.0193708\n",
      "[8,   300] loss: 0.0192952\n",
      "[8,   400] loss: 0.0192779\n",
      "[8,   500] loss: 0.0190296\n",
      "[8,   600] loss: 0.0190766\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6503,  nmi: 0.6181  <==|\n",
      "[9,   100] loss: 0.0189668\n",
      "[9,   200] loss: 0.0188216\n",
      "[9,   300] loss: 0.0189121\n",
      "[9,   400] loss: 0.0185693\n",
      "[9,   500] loss: 0.0186374\n",
      "[9,   600] loss: 0.0185084\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6703,  nmi: 0.6306  <==|\n",
      "[10,   100] loss: 0.0183777\n",
      "[10,   200] loss: 0.0183989\n",
      "[10,   300] loss: 0.0182853\n",
      "[10,   400] loss: 0.0182179\n",
      "[10,   500] loss: 0.0181473\n",
      "[10,   600] loss: 0.0180166\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6742,  nmi: 0.6353  <==|\n",
      "Initializing cluster center with pre-trained weights\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6742,  nmi: 0.6353  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6741,  nmi: 0.6355  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6749,  nmi: 0.6422  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6635,  nmi: 0.6347  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6618,  nmi: 0.6372  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6584,  nmi: 0.6310  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6586,  nmi: 0.6361  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6614,  nmi: 0.6365  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6618,  nmi: 0.6412  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6641,  nmi: 0.6438  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6646,  nmi: 0.6460  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6633,  nmi: 0.6458  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6638,  nmi: 0.6473  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6663,  nmi: 0.6521  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6653,  nmi: 0.6526  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6685,  nmi: 0.6550  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6673,  nmi: 0.6551  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6670,  nmi: 0.6563  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6688,  nmi: 0.6587  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6667,  nmi: 0.6556  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6704,  nmi: 0.6626  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6698,  nmi: 0.6636  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6701,  nmi: 0.6630  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6740,  nmi: 0.6667  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6716,  nmi: 0.6661  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6730,  nmi: 0.6676  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6712,  nmi: 0.6659  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6726,  nmi: 0.6685  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6720,  nmi: 0.6692  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6746,  nmi: 0.6720  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6729,  nmi: 0.6696  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6744,  nmi: 0.6723  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6749,  nmi: 0.6725  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6763,  nmi: 0.6754  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6738,  nmi: 0.6745  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6771,  nmi: 0.6767  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6777,  nmi: 0.6780  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6777,  nmi: 0.6777  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6777,  nmi: 0.6766  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6800,  nmi: 0.6817  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6769,  nmi: 0.6793  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6807,  nmi: 0.6833  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6814,  nmi: 0.6829  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6781,  nmi: 0.6793  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6805,  nmi: 0.6814  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6799,  nmi: 0.6817  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6824,  nmi: 0.6843  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6810,  nmi: 0.6836  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6825,  nmi: 0.6864  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6806,  nmi: 0.6839  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6816,  nmi: 0.6851  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6830,  nmi: 0.6861  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6809,  nmi: 0.6848  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6826,  nmi: 0.6894  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6833,  nmi: 0.6892  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6827,  nmi: 0.6867  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6822,  nmi: 0.6861  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6833,  nmi: 0.6875  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6833,  nmi: 0.6892  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6831,  nmi: 0.6896  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6859,  nmi: 0.6934  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6847,  nmi: 0.6919  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6843,  nmi: 0.6920  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6846,  nmi: 0.6905  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6850,  nmi: 0.6911  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6866,  nmi: 0.6949  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6847,  nmi: 0.6935  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6832,  nmi: 0.6874  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6851,  nmi: 0.6934  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6863,  nmi: 0.6954  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6852,  nmi: 0.6927  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6852,  nmi: 0.6913  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6874,  nmi: 0.6943  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6864,  nmi: 0.6923  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6877,  nmi: 0.6954  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6862,  nmi: 0.6950  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6857,  nmi: 0.6948  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6866,  nmi: 0.6968  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6867,  nmi: 0.6943  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6855,  nmi: 0.6959  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6879,  nmi: 0.6984  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6879,  nmi: 0.6973  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6885,  nmi: 0.6963  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6877,  nmi: 0.6966  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6885,  nmi: 0.6968  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6888,  nmi: 0.6978  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6865,  nmi: 0.6974  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6897,  nmi: 0.7002  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6878,  nmi: 0.6990  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6896,  nmi: 0.7005  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6899,  nmi: 0.7015  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6880,  nmi: 0.6997  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6884,  nmi: 0.6999  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6888,  nmi: 0.6996  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6885,  nmi: 0.6976  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6890,  nmi: 0.6990  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6891,  nmi: 0.6998  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6900,  nmi: 0.7009  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6914,  nmi: 0.7030  <==|\n",
      "(10000, 10)\n",
      "        |==>  acc: 0.6897,  nmi: 0.6996  <==|\n"
     ]
    }
   ],
   "source": [
    "#now start training\n",
    "import random\n",
    "random.seed(7)\n",
    "dec = DEC(10)\n",
    "dec.pretrain(train_loader, test_loader, 50)\n",
    "dec.train(train_loader, test_loader, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[[1,2],[2,3]]\n",
    "np.stack(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
