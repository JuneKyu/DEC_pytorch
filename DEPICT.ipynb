{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.cluster import KMeans\n",
    "## load mnist dataset\n",
    "use_cuda = torch.cuda.is_available()\n",
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "trans = transforms.Compose([transforms.Resize(32), transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "# if not exist, download mnist dataset\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)\n",
    "batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DEC_AE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DEC_AE,self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.conv_ae1 = nn.Conv2d(1,50,4,stride=2,padding=2)\n",
    "        self.conv_ae2 = nn.Conv2d(50,50,5,stride=2,padding=2)\n",
    "        self.leReLU = nn.LeakyReLU()\n",
    "        self.fc1 = nn.Linear(50*9*9,68)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc_de = nn.Linear(68,50*9*9)\n",
    "        self.conv_de2 = nn.ConvTranspose2d(50,50,5,stride=2,padding=2)\n",
    "        self.conv_de1 = nn.ConvTranspose2d(50,1,4,stride=2,padding=2)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                import scipy.stats as stats\n",
    "                stddev = m.stddev if hasattr(m, 'stddev') else 0.1\n",
    "                X = stats.truncnorm(-2, 2, scale=stddev)\n",
    "                values = torch.Tensor(X.rvs(m.weight.numel()))\n",
    "                values = values.view(m.weight.size())\n",
    "                m.weight.data.copy_(values)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def forward(self,x):\n",
    "        # 32x32x1\n",
    "        x = self.dropout(x)\n",
    "        # 32x32x1\n",
    "        x = self.conv_ae1(x)\n",
    "        # 17x17x50\n",
    "        x = self.leReLU(x)\n",
    "        # 17x17x50\n",
    "        x = self.dropout(x)\n",
    "        # 17x17x50\n",
    "        x = self.conv_ae2(x)\n",
    "        # 9x9x50\n",
    "        x = self.leReLU(x)\n",
    "        # 9x9x50\n",
    "        x = self.dropout(x)\n",
    "        # 9x9x50\n",
    "        x = x.view(-1, 50*9*9)\n",
    "        # 1x4050\n",
    "        x = self.fc1(x)\n",
    "        # 1x68\n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        x_ae = x # this is the returned auto encoder\n",
    "        # 1x68\n",
    "        ##### auto encoder is done, followed by decoder #####\n",
    "        # 1x68\n",
    "        x = self.fc_de(x)\n",
    "        # 1x4050\n",
    "        x = self.tanh(x)\n",
    "        # 1x4050\n",
    "        x = x.view(-1,50,9,9)\n",
    "        # 9*9*50\n",
    "        x = self.conv_de2(x)\n",
    "        # 17x17x50\n",
    "        x = self.leReLU(x)\n",
    "        # 17x17x50\n",
    "        x = self.conv_de1(x)\n",
    "        # 32x32x1\n",
    "        x = self.tanh(x)\n",
    "        x_de = x # this is the returned decoder\n",
    "        \n",
    "        return x_ae, x_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BasicUnit(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicUnit,self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.conv_a1 = nn.Conv2d(1,50,4,stride=2,padding=2)\n",
    "        self.conv_a2 = nn.Conv2d(50,50,5,stride=2,padding=2)\n",
    "        self.leReLU = nn.LeakyReLU()\n",
    "        self.fca1 = nn.Linear(50*9*9,10)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                import scipy.stats as stats\n",
    "                stddev = m.stddev if hasattr(m, 'stddev') else 0.1\n",
    "                X = stats.truncnorm(-2, 2, scale=stddev)\n",
    "                values = torch.Tensor(X.rvs(m.weight.numel()))\n",
    "                values = values.view(m.weight.size())\n",
    "                m.weight.data.copy_(values)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def forward(self,x):\n",
    "        # 32x32x1\n",
    "        x = self.dropout(x)        \n",
    "        # 32x32x1\n",
    "        x = self.conv_a1(x)\n",
    "        # 17x17x50\n",
    "        x = self.leReLU(x)\n",
    "        # 17x17x50\n",
    "        x = self.dropout(x)\n",
    "        # 17x17x50\n",
    "        x = self.conv_a2(x)\n",
    "        # 9x9x50\n",
    "        x = self.leReLU(x)\n",
    "        # 9x9x50\n",
    "        x = self.dropout(x)\n",
    "        # 9x9x50\n",
    "        x = x.view(-1, 50*9*9)\n",
    "        # 1x4050\n",
    "        x = self.fca1(x)\n",
    "        # 1x68\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "class DTC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DTC,self).__init__()\n",
    "        self.unit_a = BasicUnit()\n",
    "        self.unit_b = BasicUnit()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x1 = self.unit_a(x)\n",
    "        x2 = self.unit_b(x)\n",
    "        \n",
    "        return x1,x2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "nmi = normalized_mutual_info_score\n",
    "ari = adjusted_rand_score\n",
    "\n",
    "\n",
    "def acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DEC:\n",
    "    \"\"\"The class for controlling the training process of DEC\"\"\"\n",
    "    def pretrain(self,train_loader, test_loader, epochs):\n",
    "        \n",
    "        dec_ae = DEC_AE().cuda() #auto encoder\n",
    "        mseloss = nn.MSELoss()\n",
    "        optimizer = optim.Adam(dec_ae.parameters())\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            dec_ae.train()\n",
    "            running_loss=0.0\n",
    "            for i,data in enumerate(train_loader):\n",
    "                x, label = data\n",
    "                x,label=Variable(x).cuda(),Variable(label).cuda()\n",
    "                optimizer.zero_grad()\n",
    "                x_ae,x_de = dec_ae(x)\n",
    "                loss = F.mse_loss(x_de,x).mean() #mseloss(x_de,x) # so the aim is to minimize the reconstruct error\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # print statistics\n",
    "                running_loss += loss.data.cpu().numpy()[0]\n",
    "                if i % 100 == 99:    # print every 2000 mini-batches\n",
    "                    print('[%d, %5d] loss: %.7f' %\n",
    "                          (epoch + 1, i + 1, running_loss / 100))\n",
    "                    #print('x_de:',x_de, x)\n",
    "                    running_loss = 0.0\n",
    "            #now we evaluate the accuracy with AE\n",
    "            dec_ae.eval()\n",
    "            for i,data in enumerate(test_loader):\n",
    "                x, label = data\n",
    "                x=Variable(x).cuda()\n",
    "                x_ae,_ = dec_ae(x)\n",
    "                x_ae = x_ae.data.cpu().numpy()\n",
    "                label = label.cpu().numpy()\n",
    "                km = KMeans(n_clusters=len(np.unique(label)), n_init=20, n_jobs=4)\n",
    "                y_pred = km.fit_predict(x_ae)\n",
    "                print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
    "                          % (acc(label, y_pred), nmi(label, y_pred)))\n",
    "                break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lxp/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: -0.1554796\n",
      "[1,   200] loss: -0.1554712\n",
      "[1,   300] loss: -0.1556141\n",
      "[1,   400] loss: -0.1555810\n",
      "[1,   500] loss: -0.1560651\n",
      "[1,   600] loss: -0.1554027\n",
      "y_pred [0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 4 1 1 0 0 0 0 0 0 0 4 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 4 0 1 0 4 0 0 0 0 0 4 0 0 0 0 0 4 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "        |==>  acc: 0.2200,  nmi: 0.1432  <==|\n",
      "[2,   100] loss: -0.1554729\n",
      "[2,   200] loss: -0.1555303\n",
      "[2,   300] loss: -0.1559069\n",
      "[2,   400] loss: -0.1553018\n",
      "[2,   500] loss: -0.1555729\n",
      "[2,   600] loss: -0.1557736\n",
      "y_pred [0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 4 1 1 0 0 0 0 0 0 0 4 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 4 0 1 0 4 0 0 0 0 0 4 0 0 0 0 0 4 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "        |==>  acc: 0.2200,  nmi: 0.1432  <==|\n",
      "[3,   100] loss: -0.1558164\n",
      "[3,   200] loss: -0.1556884\n",
      "[3,   300] loss: -0.1558963\n",
      "[3,   400] loss: -0.1556924\n",
      "[3,   500] loss: -0.1560506\n",
      "[3,   600] loss: -0.1558939\n",
      "y_pred [0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 4 1 1 0 0 0 0 0 0 0 4 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 4 0 1 0 4 0 0 0 0 0 4 0 0 0 0 0 4 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "        |==>  acc: 0.2200,  nmi: 0.1432  <==|\n",
      "[4,   100] loss: -0.1557133\n",
      "[4,   200] loss: -0.1554272\n",
      "[4,   300] loss: -0.1560514\n",
      "[4,   400] loss: -0.1562984\n",
      "[4,   500] loss: -0.1561968\n",
      "[4,   600] loss: -0.1552357\n",
      "y_pred [0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 4 1 1 0 0 0 0 0 0 0 4 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 4 0 1 0 4 0 0 0 0 0 4 0 0 0 0 0 4 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "        |==>  acc: 0.2200,  nmi: 0.1432  <==|\n",
      "[5,   100] loss: -0.1557550\n",
      "[5,   200] loss: -0.1555605\n",
      "[5,   300] loss: -0.1565508\n",
      "[5,   400] loss: -0.1559304\n",
      "[5,   500] loss: -0.1555031\n",
      "[5,   600] loss: -0.1560799\n",
      "y_pred [0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 4 1 1 0 0 0 0 0 0 0 4 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 4 0 1 0 4 0 0 0 0 0 4 0 0 0 0 0 4 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "        |==>  acc: 0.2200,  nmi: 0.1432  <==|\n",
      "[6,   100] loss: -0.1556647\n",
      "[6,   200] loss: -0.1562467\n",
      "[6,   300] loss: -0.1555972\n",
      "[6,   400] loss: -0.1550870\n",
      "[6,   500] loss: -0.1558505\n",
      "[6,   600] loss: -0.1552285\n",
      "y_pred [0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 4 1 1 0 0 0 0 0 0 0 4 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 4 0 1 0 4 0 0 0 0 0 4 0 0 0 0 0 4 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "        |==>  acc: 0.2200,  nmi: 0.1432  <==|\n",
      "[7,   100] loss: -0.1564328\n",
      "[7,   200] loss: -0.1556237\n",
      "[7,   300] loss: -0.1557567\n",
      "[7,   400] loss: -0.1555116\n",
      "[7,   500] loss: -0.1559159\n",
      "[7,   600] loss: -0.1559007\n",
      "y_pred [0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 4 1 1 0 0 0 0 0 0 0 4 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 4 0 1 0 4 0 0 0 0 0 4 0 0 0 0 0 4 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "        |==>  acc: 0.2200,  nmi: 0.1432  <==|\n",
      "[8,   100] loss: -0.1563783\n",
      "[8,   200] loss: -0.1551558\n",
      "[8,   300] loss: -0.1559756\n",
      "[8,   400] loss: -0.1557226\n",
      "[8,   500] loss: -0.1558110\n",
      "[8,   600] loss: -0.1555730\n",
      "y_pred [0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 4 1 1 0 0 0 0 0 0 0 4 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 4 0 1 0 4 0 0 0 0 0 4 0 0 0 0 0 4 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "        |==>  acc: 0.2200,  nmi: 0.1432  <==|\n",
      "[9,   100] loss: -0.1558395\n",
      "[9,   200] loss: -0.1554485\n",
      "[9,   300] loss: -0.1553294\n",
      "[9,   400] loss: -0.1554433\n",
      "[9,   500] loss: -0.1561202\n",
      "[9,   600] loss: -0.1554876\n",
      "y_pred [0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 4 1 1 0 0 0 0 0 0 0 4 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 4 0 1 0 4 0 0 0 0 0 4 0 0 0 0 0 4 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "        |==>  acc: 0.2200,  nmi: 0.1432  <==|\n",
      "[10,   100] loss: -0.1556146\n",
      "[10,   200] loss: -0.1554588\n",
      "[10,   300] loss: -0.1562333\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1d179d454e24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mdtc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDTC_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mdtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-1d179d454e24>\u001b[0m in \u001b[0;36mpretrain\u001b[0;34m(self, train_loader, test_loader, epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mdtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mrunning_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/torchvision-0.2.0-py3.5.egg/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/torchvision-0.2.0-py3.5.egg/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/torchvision-0.2.0-py3.5.egg/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \"\"\"\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/torchvision-0.2.0-py3.5.egg/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;31m# PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'YCbCr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mtobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         \u001b[0mbufsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m65536\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# see RawEncode.c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class DTC_trainer:\n",
    "    \"\"\"The class for controlling the training process of DEC\"\"\"\n",
    "    \n",
    "    \n",
    "    def target_distribution(self,q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return Variable(((weight.t() / weight.sum(1)).t()).data,requires_grad=False)\n",
    "    \n",
    "    def pretrain(self,train_loader, test_loader, epochs):\n",
    "        \n",
    "        dtc = DTC().cuda() #auto encoder\n",
    "        mseloss = nn.MSELoss()\n",
    "        optimizer = optim.Adam(dtc.parameters())\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            dtc.train()\n",
    "            running_loss=0.0\n",
    "            for i,data in enumerate(train_loader):\n",
    "                x, label = data\n",
    "                x,label=Variable(x).cuda(),Variable(label).cuda()\n",
    "                optimizer.zero_grad()\n",
    "                x_1,x_2 = dtc(x)\n",
    "                td = self.target_distribution(x_1)\n",
    "                loss =  F.kl_div(x_2,td,reduce=True) #mseloss(x_de,x) # so the aim is to minimize the reconstruct error\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # print statistics\n",
    "                running_loss += loss.data.cpu().numpy()[0]\n",
    "                if i % 100 == 99:    # print every 2000 mini-batches\n",
    "                    print('[%d, %5d] loss: %.7f' %\n",
    "                          (epoch + 1, i + 1, running_loss / 100))\n",
    "                    #print('x_de:',x_de, x)\n",
    "                    running_loss = 0.0\n",
    "            #now we evaluate the accuracy with AE\n",
    "            dtc.eval()\n",
    "            for i,data in enumerate(test_loader):\n",
    "                x, label = data\n",
    "                x=Variable(x).cuda()\n",
    "                x_ae,_ = dtc(x)\n",
    "                x_ae = x_ae.data.cpu().numpy()\n",
    "                label = label.cpu().numpy()\n",
    "                y_pred = np.argmax(x_ae,axis=1)\n",
    "                print('y_pred',y_pred)\n",
    "                print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
    "                          % (acc(label, y_pred), nmi(label, y_pred)))\n",
    "                break\n",
    "import random\n",
    "random.seed(7)\n",
    "dtc = DTC_trainer()\n",
    "dtc.pretrain(train_loader, test_loader, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.0645982\n",
      "[1,   200] loss: 0.0546203\n",
      "[1,   300] loss: 0.0410921\n",
      "[1,   400] loss: 0.0157458\n",
      "[1,   500] loss: 0.0102162\n",
      "[1,   600] loss: 0.0079046\n",
      "        |==>  acc: 0.6000,  nmi: 0.6467  <==|\n",
      "[2,   100] loss: 0.0066989\n",
      "[2,   200] loss: 0.0059005\n",
      "[2,   300] loss: 0.0052914\n",
      "[2,   400] loss: 0.0048581\n",
      "[2,   500] loss: 0.0045226\n",
      "[2,   600] loss: 0.0042733\n",
      "        |==>  acc: 0.5700,  nmi: 0.6122  <==|\n",
      "[3,   100] loss: 0.0040128\n",
      "[3,   200] loss: 0.0038080\n",
      "[3,   300] loss: 0.0036345\n",
      "[3,   400] loss: 0.0034556\n",
      "[3,   500] loss: 0.0033510\n",
      "[3,   600] loss: 0.0032324\n",
      "        |==>  acc: 0.5300,  nmi: 0.5939  <==|\n",
      "[4,   100] loss: 0.0031155\n",
      "[4,   200] loss: 0.0030300\n",
      "[4,   300] loss: 0.0029553\n",
      "[4,   400] loss: 0.0028632\n",
      "[4,   500] loss: 0.0028077\n",
      "[4,   600] loss: 0.0027246\n",
      "        |==>  acc: 0.5100,  nmi: 0.5617  <==|\n",
      "[5,   100] loss: 0.0026454\n",
      "[5,   200] loss: 0.0026348\n",
      "[5,   300] loss: 0.0025579\n",
      "[5,   400] loss: 0.0025043\n",
      "[5,   500] loss: 0.0024836\n",
      "[5,   600] loss: 0.0024285\n",
      "        |==>  acc: 0.5600,  nmi: 0.6013  <==|\n",
      "[6,   100] loss: 0.0023903\n",
      "[6,   200] loss: 0.0023474\n",
      "[6,   300] loss: 0.0023229\n",
      "[6,   400] loss: 0.0022970\n",
      "[6,   500] loss: 0.0022475\n",
      "[6,   600] loss: 0.0022478\n",
      "        |==>  acc: 0.5200,  nmi: 0.5867  <==|\n",
      "[7,   100] loss: 0.0021944\n",
      "[7,   200] loss: 0.0021817\n",
      "[7,   300] loss: 0.0021501\n",
      "[7,   400] loss: 0.0021257\n",
      "[7,   500] loss: 0.0021087\n",
      "[7,   600] loss: 0.0020846\n",
      "        |==>  acc: 0.5400,  nmi: 0.5782  <==|\n",
      "[8,   100] loss: 0.0020413\n",
      "[8,   200] loss: 0.0020343\n",
      "[8,   300] loss: 0.0019951\n",
      "[8,   400] loss: 0.0020132\n",
      "[8,   500] loss: 0.0019828\n",
      "[8,   600] loss: 0.0019843\n",
      "        |==>  acc: 0.5800,  nmi: 0.6262  <==|\n",
      "[9,   100] loss: 0.0019228\n",
      "[9,   200] loss: 0.0019316\n",
      "[9,   300] loss: 0.0019086\n",
      "[9,   400] loss: 0.0019113\n",
      "[9,   500] loss: 0.0019010\n",
      "[9,   600] loss: 0.0018651\n",
      "        |==>  acc: 0.5600,  nmi: 0.5967  <==|\n",
      "[10,   100] loss: 0.0018321\n",
      "[10,   200] loss: 0.0018459\n",
      "[10,   300] loss: 0.0018412\n",
      "[10,   400] loss: 0.0018167\n",
      "[10,   500] loss: 0.0018125\n",
      "[10,   600] loss: 0.0017917\n",
      "        |==>  acc: 0.5900,  nmi: 0.6334  <==|\n",
      "[11,   100] loss: 0.0017840\n",
      "[11,   200] loss: 0.0017888\n",
      "[11,   300] loss: 0.0017482\n",
      "[11,   400] loss: 0.0017592\n",
      "[11,   500] loss: 0.0017356\n",
      "[11,   600] loss: 0.0017430\n",
      "        |==>  acc: 0.5200,  nmi: 0.5594  <==|\n",
      "[12,   100] loss: 0.0017061\n",
      "[12,   200] loss: 0.0016980\n",
      "[12,   300] loss: 0.0017136\n",
      "[12,   400] loss: 0.0016860\n",
      "[12,   500] loss: 0.0016945\n",
      "[12,   600] loss: 0.0016887\n",
      "        |==>  acc: 0.4800,  nmi: 0.5305  <==|\n",
      "[13,   100] loss: 0.0016613\n",
      "[13,   200] loss: 0.0016607\n",
      "[13,   300] loss: 0.0016364\n",
      "[13,   400] loss: 0.0016487\n",
      "[13,   500] loss: 0.0016454\n",
      "[13,   600] loss: 0.0016096\n",
      "        |==>  acc: 0.5400,  nmi: 0.6097  <==|\n",
      "[14,   100] loss: 0.0016133\n",
      "[14,   200] loss: 0.0016018\n",
      "[14,   300] loss: 0.0015902\n",
      "[14,   400] loss: 0.0016087\n",
      "[14,   500] loss: 0.0016021\n",
      "[14,   600] loss: 0.0015874\n",
      "        |==>  acc: 0.5000,  nmi: 0.5382  <==|\n",
      "[15,   100] loss: 0.0015708\n",
      "[15,   200] loss: 0.0015739\n",
      "[15,   300] loss: 0.0015696\n",
      "[15,   400] loss: 0.0015643\n",
      "[15,   500] loss: 0.0015515\n",
      "[15,   600] loss: 0.0015537\n",
      "        |==>  acc: 0.4900,  nmi: 0.5375  <==|\n",
      "[16,   100] loss: 0.0015430\n",
      "[16,   200] loss: 0.0015370\n",
      "[16,   300] loss: 0.0015268\n",
      "[16,   400] loss: 0.0015353\n",
      "[16,   500] loss: 0.0015283\n",
      "[16,   600] loss: 0.0015355\n",
      "        |==>  acc: 0.4900,  nmi: 0.5190  <==|\n",
      "[17,   100] loss: 0.0015056\n",
      "[17,   200] loss: 0.0015071\n",
      "[17,   300] loss: 0.0015116\n",
      "[17,   400] loss: 0.0015077\n",
      "[17,   500] loss: 0.0015102\n",
      "[17,   600] loss: 0.0014880\n",
      "        |==>  acc: 0.5200,  nmi: 0.5712  <==|\n",
      "[18,   100] loss: 0.0014925\n",
      "[18,   200] loss: 0.0014826\n",
      "[18,   300] loss: 0.0014853\n",
      "[18,   400] loss: 0.0014957\n",
      "[18,   500] loss: 0.0014695\n",
      "[18,   600] loss: 0.0014727\n",
      "        |==>  acc: 0.4900,  nmi: 0.5577  <==|\n",
      "[19,   100] loss: 0.0014635\n",
      "[19,   200] loss: 0.0014582\n",
      "[19,   300] loss: 0.0014511\n",
      "[19,   400] loss: 0.0014638\n",
      "[19,   500] loss: 0.0014664\n",
      "[19,   600] loss: 0.0014598\n",
      "        |==>  acc: 0.5300,  nmi: 0.5838  <==|\n",
      "[20,   100] loss: 0.0014344\n",
      "[20,   200] loss: 0.0014382\n",
      "[20,   300] loss: 0.0014479\n",
      "[20,   400] loss: 0.0014397\n",
      "[20,   500] loss: 0.0014278\n",
      "[20,   600] loss: 0.0014447\n",
      "        |==>  acc: 0.4800,  nmi: 0.5512  <==|\n"
     ]
    }
   ],
   "source": [
    "#now start training\n",
    "import random\n",
    "random.seed(7)\n",
    "dec = DEC()\n",
    "dec.pretrain(train_loader, test_loader, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtc = DTC()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
