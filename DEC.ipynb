{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "## load mnist dataset\n",
    "use_cuda = torch.cuda.is_available()\n",
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "#trans = transforms.Compose([transforms.Resize(32), transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "# if not exist, download mnist dataset\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)\n",
    "batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timingResult = {}\n",
    "def logTime(theName, currentTime):\n",
    "    if theName not in timingResult:\n",
    "        timingResult[theName] = time.time() - currentTime\n",
    "    else:\n",
    "        timingResult[theName] = timingResult[theName] + (time.time() - currentTime)\n",
    "    currentTime = time.time()\n",
    "    return currentTime\n",
    "\n",
    "def printTiming(name):\n",
    "    print('======== timing for {}: {} ======='.format(name,timingResult[name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEC_AE(nn.Module):\n",
    "    def __init__(self, num_classes, num_features):\n",
    "        super(DEC_AE,self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.fc1 = nn.Linear(28*28,500)\n",
    "        self.fc2 = nn.Linear(500,500)\n",
    "        self.fc3 = nn.Linear(500,2000)\n",
    "        self.fc4 = nn.Linear(2000,num_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_d1 = nn.Linear(500,28*28)\n",
    "        self.fc_d2 = nn.Linear(500,500)\n",
    "        self.fc_d3 = nn.Linear(2000,500)\n",
    "        self.fc_d4 = nn.Linear(num_features,2000)\n",
    "        self.alpha = 1.0\n",
    "        self.clusterCenter = nn.Parameter(torch.zeros(num_classes,num_features))\n",
    "        self.pretrainMode = True\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform(m.weight)\n",
    "\n",
    "    def setPretrain(self,mode):\n",
    "        \"\"\"To set training mode to pretrain or not, \n",
    "        so that it can control to run only the Encoder or Encoder+Decoder\"\"\"\n",
    "        self.pretrainMode = mode\n",
    "    def updateClusterCenter(self, cc):\n",
    "        \"\"\"\n",
    "        To update the cluster center. This is a method for pre-train phase.\n",
    "        When a center is being provided by kmeans, we need to update it so\n",
    "        that it is available for further training\n",
    "        :param cc: the cluster centers to update, size of num_classes x num_features\n",
    "        \"\"\"\n",
    "        self.clusterCenter.data = torch.from_numpy(cc)\n",
    "    def getTDistribution(self,x, clusterCenter):\n",
    "        \"\"\"\n",
    "        student t-distribution, as same as used in t-SNE algorithm.\n",
    "         q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "         \n",
    "         :param x: input data, in this context it is encoder output\n",
    "         :param clusterCenter: the cluster center from kmeans\n",
    "         \"\"\"\n",
    "        xe = torch.unsqueeze(x,1).cuda() - clusterCenter.cuda()\n",
    "        q = 1.0 / (1.0 + (torch.sum(torch.mul(xe,xe), 2) / self.alpha))\n",
    "        q = q ** (self.alpha + 1.0) / 2.0\n",
    "        q = (q.t() / torch.sum(q, 1)).t() #due to divison, we need to transpose q\n",
    "        return q\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(-1, 1*28*28)\n",
    "        # 32x32x1\n",
    "        x = self.dropout(x)\n",
    "        # 32x32x1\n",
    "        x = self.fc1(x)\n",
    "        # 17x17x50\n",
    "        x = self.relu(x)\n",
    "        # 17x17x50\n",
    "        x = self.fc2(x)\n",
    "        # 17x17x50\n",
    "        x = self.relu(x)\n",
    "        # 9x9x50\n",
    "        x = self.fc3(x)\n",
    "        # 17x17x50\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        # 9x9x50\n",
    "        x_ae = x\n",
    "        #if not in pretrain mode, we only need encoder\n",
    "        if self.pretrainMode == False:\n",
    "            return x, self.getTDistribution(x,self.clusterCenter)\n",
    "        # 1x68\n",
    "        ##### encoder is done, followed by decoder #####\n",
    "        # 1x68\n",
    "        x = self.fc_d4(x)\n",
    "        # 1x4050\n",
    "        x = self.relu(x)\n",
    "        # 1x4050\n",
    "        x = self.fc_d3(x)\n",
    "        # 1x4050\n",
    "        x = self.relu(x)\n",
    "        x = self.fc_d2(x)\n",
    "        # 1x4050\n",
    "        x = self.relu(x)\n",
    "        x = self.fc_d1(x)\n",
    "        x_de = x.view(-1,1,28,28)\n",
    "        # 1x4050\n",
    "        return x_ae, x_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "nmi = normalized_mutual_info_score\n",
    "ari = adjusted_rand_score\n",
    "\n",
    "\n",
    "def acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEC:\n",
    "    \"\"\"The class for controlling the training process of DEC\"\"\"\n",
    "    def __init__(self,n_clusters,alpha=1.0):\n",
    "        self.n_clusters=n_clusters\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    @staticmethod\n",
    "    def target_distribution(q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        #print('q',q)\n",
    "        return Variable((weight.t() / weight.sum(1)).t().data, requires_grad=True)\n",
    "    def logAccuracy(self,pred,label):\n",
    "        print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
    "          % (acc(label, pred), nmi(label, pred)))\n",
    "    @staticmethod\n",
    "    def kld(q,p):\n",
    "        res = torch.sum(p*torch.log(p/q),dim=-1)\n",
    "        return res\n",
    "    \n",
    "    def validateOnCompleteTestData(self,test_loader,model):\n",
    "        model.eval()\n",
    "        to_eval = np.array([model(d[0].cuda())[0].data.cpu().numpy() for i,d in enumerate(test_loader)])\n",
    "        true_labels = np.array([d[1].cpu().numpy() for i,d in enumerate(test_loader)])\n",
    "        to_eval = np.reshape(to_eval,(to_eval.shape[0]*to_eval.shape[1],to_eval.shape[2]))\n",
    "        true_labels = np.reshape(true_labels,true_labels.shape[0]*true_labels.shape[1])\n",
    "        km = KMeans(n_clusters=len(np.unique(true_labels)), n_init=20, n_jobs=4)\n",
    "        y_pred = km.fit_predict(to_eval)\n",
    "        print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
    "                      % (acc(true_labels, y_pred), nmi(true_labels, y_pred)))\n",
    "        currentAcc = acc(true_labels, y_pred)\n",
    "        return currentAcc\n",
    "    \n",
    "    def pretrain(self,train_loader, test_loader, epochs):\n",
    "        dec_ae = DEC_AE(10,10).cuda() #auto encoder\n",
    "        mseloss = nn.MSELoss()\n",
    "        optimizer = optim.SGD(dec_ae.parameters(),lr = 1, momentum=0.9)\n",
    "        best_acc = 0.0\n",
    "        for epoch in range(epochs):\n",
    "            dec_ae.train()\n",
    "            running_loss=0.0\n",
    "            for i,data in enumerate(train_loader):\n",
    "                x, label = data\n",
    "                x, label = Variable(x).cuda(),Variable(label).cuda()\n",
    "                optimizer.zero_grad()\n",
    "                x_ae,x_de = dec_ae(x)\n",
    "                loss = F.mse_loss(x_de,x,reduce=True) \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                x_eval = x.data.cpu().numpy()\n",
    "                label_eval = label.data.cpu().numpy()\n",
    "                running_loss += loss.data.cpu().numpy()[0]\n",
    "                if i % 100 == 99:    # print every 100 mini-batches\n",
    "                    print('[%d, %5d] loss: %.7f' %\n",
    "                          (epoch + 1, i + 1, running_loss / 100))\n",
    "                    running_loss = 0.0\n",
    "            #now we evaluate the accuracy with AE\n",
    "            dec_ae.eval()\n",
    "            currentAcc = self.validateOnCompleteTestData(test_loader,dec_ae)\n",
    "            if currentAcc > best_acc:                \n",
    "                torch.save(dec_ae,'bestModel'.format(best_acc))\n",
    "                best_acc = currentAcc\n",
    "    def clustering(self,mbk,x,model):\n",
    "        model.eval()\n",
    "        y_pred_ae,_ = model(x)\n",
    "        y_pred_ae = y_pred_ae.data.cpu().numpy()\n",
    "        y_pred = mbk.partial_fit(y_pred_ae) #seems we can only get a centre from batch\n",
    "        self.cluster_centers = mbk.cluster_centers_ #keep the cluster centers\n",
    "        model.updateClusterCenter(self.cluster_centers)\n",
    "    def train(self,train_loader, test_loader, epochs):\n",
    "        \"\"\"This method will start training for DEC cluster\"\"\"\n",
    "        ct = time.time()\n",
    "        model = torch.load(\"bestModel\").cuda()\n",
    "        model.setPretrain(False)\n",
    "        optimizer = optim.SGD([\\\n",
    "             {'params': model.parameters()}, \\\n",
    "            ],lr = 0.01, momentum=0.9)\n",
    "        print('Initializing cluster center with pre-trained weights')\n",
    "        mbk = MiniBatchKMeans(n_clusters=self.n_clusters, n_init=20, batch_size=batch_size)\n",
    "        got_cluster_center = False\n",
    "        for epoch in range(epochs):\n",
    "            for i,data in enumerate(train_loader):\n",
    "                x, label = data\n",
    "                x = Variable(x).cuda()\n",
    "                optimizer.zero_grad()\n",
    "                #step 1 - get cluster center from batch\n",
    "                #here we are using minibatch kmeans to be able to cope with larger dataset.\n",
    "                if not got_cluster_center:\n",
    "                    self.clustering(mbk,x,model)\n",
    "                    if epoch > 1:\n",
    "                        got_cluster_center = True\n",
    "                else:\n",
    "                    model.train()\n",
    "                    #now we start training with acquired cluster center\n",
    "                    feature_pred,q = model(x)\n",
    "                    #get target distribution\n",
    "                    p = self.target_distribution(q)\n",
    "                    #print('q',q,'p',p)\n",
    "                    loss = self.kld(q,p).mean()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            currentAcc = self.validateOnCompleteTestData(test_loader,model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing cluster center with pre-trained weights\n",
      "        |==>  acc: 0.8434,  nmi: 0.7760  <==|\n",
      "        |==>  acc: 0.8415,  nmi: 0.7748  <==|\n",
      "        |==>  acc: 0.8543,  nmi: 0.8295  <==|\n",
      "        |==>  acc: 0.8629,  nmi: 0.8423  <==|\n",
      "        |==>  acc: 0.8663,  nmi: 0.8458  <==|\n",
      "        |==>  acc: 0.8677,  nmi: 0.8521  <==|\n",
      "        |==>  acc: 0.8696,  nmi: 0.8550  <==|\n",
      "        |==>  acc: 0.8715,  nmi: 0.8586  <==|\n",
      "        |==>  acc: 0.8733,  nmi: 0.8600  <==|\n",
      "        |==>  acc: 0.8734,  nmi: 0.8600  <==|\n",
      "        |==>  acc: 0.8714,  nmi: 0.8575  <==|\n",
      "        |==>  acc: 0.8745,  nmi: 0.8625  <==|\n",
      "        |==>  acc: 0.8736,  nmi: 0.8622  <==|\n",
      "        |==>  acc: 0.8736,  nmi: 0.8622  <==|\n",
      "        |==>  acc: 0.8742,  nmi: 0.8621  <==|\n",
      "        |==>  acc: 0.8725,  nmi: 0.8615  <==|\n",
      "        |==>  acc: 0.8741,  nmi: 0.8638  <==|\n",
      "        |==>  acc: 0.8726,  nmi: 0.8610  <==|\n",
      "        |==>  acc: 0.8738,  nmi: 0.8649  <==|\n",
      "        |==>  acc: 0.8744,  nmi: 0.8626  <==|\n",
      "        |==>  acc: 0.8758,  nmi: 0.8660  <==|\n",
      "        |==>  acc: 0.8769,  nmi: 0.8683  <==|\n",
      "        |==>  acc: 0.8761,  nmi: 0.8668  <==|\n",
      "        |==>  acc: 0.8762,  nmi: 0.8674  <==|\n",
      "        |==>  acc: 0.8759,  nmi: 0.8666  <==|\n",
      "        |==>  acc: 0.8743,  nmi: 0.8662  <==|\n",
      "        |==>  acc: 0.8755,  nmi: 0.8662  <==|\n",
      "        |==>  acc: 0.8760,  nmi: 0.8672  <==|\n",
      "        |==>  acc: 0.8770,  nmi: 0.8686  <==|\n",
      "        |==>  acc: 0.8757,  nmi: 0.8654  <==|\n",
      "        |==>  acc: 0.8757,  nmi: 0.8656  <==|\n",
      "        |==>  acc: 0.8760,  nmi: 0.8670  <==|\n",
      "        |==>  acc: 0.8765,  nmi: 0.8669  <==|\n",
      "        |==>  acc: 0.8762,  nmi: 0.8658  <==|\n",
      "        |==>  acc: 0.8767,  nmi: 0.8659  <==|\n",
      "        |==>  acc: 0.8772,  nmi: 0.8689  <==|\n",
      "        |==>  acc: 0.8759,  nmi: 0.8667  <==|\n",
      "        |==>  acc: 0.8754,  nmi: 0.8654  <==|\n",
      "        |==>  acc: 0.8771,  nmi: 0.8683  <==|\n",
      "        |==>  acc: 0.8758,  nmi: 0.8663  <==|\n",
      "        |==>  acc: 0.8751,  nmi: 0.8656  <==|\n",
      "        |==>  acc: 0.8754,  nmi: 0.8670  <==|\n",
      "        |==>  acc: 0.8764,  nmi: 0.8678  <==|\n",
      "        |==>  acc: 0.8730,  nmi: 0.8640  <==|\n",
      "        |==>  acc: 0.8743,  nmi: 0.8668  <==|\n",
      "        |==>  acc: 0.8746,  nmi: 0.8673  <==|\n",
      "        |==>  acc: 0.8736,  nmi: 0.8656  <==|\n",
      "        |==>  acc: 0.8752,  nmi: 0.8674  <==|\n",
      "        |==>  acc: 0.8759,  nmi: 0.8682  <==|\n",
      "        |==>  acc: 0.8743,  nmi: 0.8645  <==|\n",
      "        |==>  acc: 0.8747,  nmi: 0.8667  <==|\n",
      "        |==>  acc: 0.8751,  nmi: 0.8677  <==|\n",
      "        |==>  acc: 0.8744,  nmi: 0.8661  <==|\n",
      "        |==>  acc: 0.8756,  nmi: 0.8695  <==|\n",
      "        |==>  acc: 0.8751,  nmi: 0.8681  <==|\n",
      "        |==>  acc: 0.8752,  nmi: 0.8675  <==|\n",
      "        |==>  acc: 0.8753,  nmi: 0.8696  <==|\n",
      "        |==>  acc: 0.8753,  nmi: 0.8693  <==|\n",
      "        |==>  acc: 0.8758,  nmi: 0.8696  <==|\n",
      "        |==>  acc: 0.8754,  nmi: 0.8679  <==|\n",
      "        |==>  acc: 0.8757,  nmi: 0.8678  <==|\n",
      "        |==>  acc: 0.8747,  nmi: 0.8683  <==|\n",
      "        |==>  acc: 0.8751,  nmi: 0.8678  <==|\n",
      "        |==>  acc: 0.8747,  nmi: 0.8671  <==|\n",
      "        |==>  acc: 0.8743,  nmi: 0.8672  <==|\n",
      "        |==>  acc: 0.8732,  nmi: 0.8641  <==|\n",
      "        |==>  acc: 0.8751,  nmi: 0.8677  <==|\n",
      "        |==>  acc: 0.8758,  nmi: 0.8682  <==|\n",
      "        |==>  acc: 0.8745,  nmi: 0.8663  <==|\n",
      "        |==>  acc: 0.8757,  nmi: 0.8680  <==|\n",
      "        |==>  acc: 0.8747,  nmi: 0.8672  <==|\n",
      "        |==>  acc: 0.8758,  nmi: 0.8688  <==|\n",
      "        |==>  acc: 0.8744,  nmi: 0.8675  <==|\n",
      "        |==>  acc: 0.8750,  nmi: 0.8678  <==|\n",
      "        |==>  acc: 0.8739,  nmi: 0.8648  <==|\n",
      "        |==>  acc: 0.8752,  nmi: 0.8674  <==|\n",
      "        |==>  acc: 0.8738,  nmi: 0.8656  <==|\n",
      "        |==>  acc: 0.8730,  nmi: 0.8641  <==|\n",
      "        |==>  acc: 0.8750,  nmi: 0.8662  <==|\n",
      "        |==>  acc: 0.8747,  nmi: 0.8671  <==|\n",
      "        |==>  acc: 0.8738,  nmi: 0.8665  <==|\n",
      "        |==>  acc: 0.8744,  nmi: 0.8671  <==|\n",
      "        |==>  acc: 0.8742,  nmi: 0.8665  <==|\n",
      "        |==>  acc: 0.8740,  nmi: 0.8661  <==|\n",
      "        |==>  acc: 0.8747,  nmi: 0.8680  <==|\n",
      "        |==>  acc: 0.8736,  nmi: 0.8650  <==|\n",
      "        |==>  acc: 0.8747,  nmi: 0.8677  <==|\n",
      "        |==>  acc: 0.8743,  nmi: 0.8667  <==|\n",
      "        |==>  acc: 0.8742,  nmi: 0.8655  <==|\n",
      "        |==>  acc: 0.8747,  nmi: 0.8672  <==|\n",
      "        |==>  acc: 0.8745,  nmi: 0.8659  <==|\n",
      "        |==>  acc: 0.8746,  nmi: 0.8661  <==|\n",
      "        |==>  acc: 0.8758,  nmi: 0.8675  <==|\n",
      "        |==>  acc: 0.8757,  nmi: 0.8675  <==|\n",
      "        |==>  acc: 0.8753,  nmi: 0.8668  <==|\n",
      "        |==>  acc: 0.8755,  nmi: 0.8673  <==|\n",
      "        |==>  acc: 0.8756,  nmi: 0.8676  <==|\n",
      "        |==>  acc: 0.8757,  nmi: 0.8667  <==|\n",
      "        |==>  acc: 0.8736,  nmi: 0.8652  <==|\n",
      "        |==>  acc: 0.8766,  nmi: 0.8693  <==|\n",
      "        |==>  acc: 0.8743,  nmi: 0.8663  <==|\n",
      "        |==>  acc: 0.8760,  nmi: 0.8680  <==|\n",
      "        |==>  acc: 0.8749,  nmi: 0.8663  <==|\n",
      "        |==>  acc: 0.8749,  nmi: 0.8660  <==|\n",
      "        |==>  acc: 0.8759,  nmi: 0.8668  <==|\n",
      "        |==>  acc: 0.8753,  nmi: 0.8656  <==|\n",
      "        |==>  acc: 0.8761,  nmi: 0.8691  <==|\n",
      "        |==>  acc: 0.8744,  nmi: 0.8645  <==|\n",
      "        |==>  acc: 0.8764,  nmi: 0.8666  <==|\n",
      "        |==>  acc: 0.8738,  nmi: 0.8637  <==|\n",
      "        |==>  acc: 0.8756,  nmi: 0.8659  <==|\n",
      "        |==>  acc: 0.8742,  nmi: 0.8647  <==|\n",
      "        |==>  acc: 0.8763,  nmi: 0.8682  <==|\n",
      "        |==>  acc: 0.8765,  nmi: 0.8674  <==|\n",
      "        |==>  acc: 0.8759,  nmi: 0.8669  <==|\n",
      "        |==>  acc: 0.8743,  nmi: 0.8655  <==|\n",
      "        |==>  acc: 0.8757,  nmi: 0.8663  <==|\n",
      "        |==>  acc: 0.8763,  nmi: 0.8675  <==|\n",
      "        |==>  acc: 0.8778,  nmi: 0.8698  <==|\n",
      "        |==>  acc: 0.8774,  nmi: 0.8690  <==|\n",
      "        |==>  acc: 0.8780,  nmi: 0.8698  <==|\n",
      "        |==>  acc: 0.8761,  nmi: 0.8665  <==|\n",
      "        |==>  acc: 0.8763,  nmi: 0.8665  <==|\n",
      "        |==>  acc: 0.8756,  nmi: 0.8671  <==|\n",
      "        |==>  acc: 0.8753,  nmi: 0.8677  <==|\n",
      "        |==>  acc: 0.8751,  nmi: 0.8674  <==|\n",
      "        |==>  acc: 0.8757,  nmi: 0.8681  <==|\n",
      "        |==>  acc: 0.8747,  nmi: 0.8643  <==|\n",
      "        |==>  acc: 0.8751,  nmi: 0.8675  <==|\n",
      "        |==>  acc: 0.8757,  nmi: 0.8661  <==|\n",
      "        |==>  acc: 0.8762,  nmi: 0.8668  <==|\n",
      "        |==>  acc: 0.8759,  nmi: 0.8665  <==|\n",
      "        |==>  acc: 0.8749,  nmi: 0.8669  <==|\n",
      "        |==>  acc: 0.8751,  nmi: 0.8662  <==|\n",
      "        |==>  acc: 0.8751,  nmi: 0.8661  <==|\n",
      "        |==>  acc: 0.8769,  nmi: 0.8683  <==|\n",
      "        |==>  acc: 0.8757,  nmi: 0.8687  <==|\n",
      "        |==>  acc: 0.8747,  nmi: 0.8670  <==|\n",
      "        |==>  acc: 0.8766,  nmi: 0.8689  <==|\n",
      "        |==>  acc: 0.8752,  nmi: 0.8654  <==|\n",
      "        |==>  acc: 0.8765,  nmi: 0.8679  <==|\n",
      "        |==>  acc: 0.8753,  nmi: 0.8659  <==|\n",
      "        |==>  acc: 0.8761,  nmi: 0.8666  <==|\n",
      "        |==>  acc: 0.8766,  nmi: 0.8692  <==|\n",
      "        |==>  acc: 0.8747,  nmi: 0.8657  <==|\n",
      "        |==>  acc: 0.8756,  nmi: 0.8676  <==|\n",
      "        |==>  acc: 0.8767,  nmi: 0.8680  <==|\n",
      "        |==>  acc: 0.8773,  nmi: 0.8687  <==|\n",
      "        |==>  acc: 0.8761,  nmi: 0.8653  <==|\n",
      "        |==>  acc: 0.8765,  nmi: 0.8659  <==|\n",
      "        |==>  acc: 0.8773,  nmi: 0.8674  <==|\n",
      "        |==>  acc: 0.8764,  nmi: 0.8673  <==|\n",
      "        |==>  acc: 0.8778,  nmi: 0.8675  <==|\n",
      "        |==>  acc: 0.8772,  nmi: 0.8670  <==|\n",
      "        |==>  acc: 0.8773,  nmi: 0.8677  <==|\n",
      "        |==>  acc: 0.8780,  nmi: 0.8678  <==|\n",
      "        |==>  acc: 0.8775,  nmi: 0.8666  <==|\n",
      "        |==>  acc: 0.8772,  nmi: 0.8667  <==|\n",
      "        |==>  acc: 0.8768,  nmi: 0.8666  <==|\n",
      "        |==>  acc: 0.8779,  nmi: 0.8667  <==|\n",
      "        |==>  acc: 0.8793,  nmi: 0.8694  <==|\n",
      "        |==>  acc: 0.8785,  nmi: 0.8688  <==|\n",
      "        |==>  acc: 0.8797,  nmi: 0.8683  <==|\n",
      "        |==>  acc: 0.8789,  nmi: 0.8695  <==|\n",
      "        |==>  acc: 0.8803,  nmi: 0.8699  <==|\n",
      "        |==>  acc: 0.8794,  nmi: 0.8683  <==|\n",
      "        |==>  acc: 0.8805,  nmi: 0.8690  <==|\n",
      "        |==>  acc: 0.8791,  nmi: 0.8679  <==|\n",
      "        |==>  acc: 0.8791,  nmi: 0.8678  <==|\n",
      "        |==>  acc: 0.8776,  nmi: 0.8663  <==|\n",
      "        |==>  acc: 0.8783,  nmi: 0.8683  <==|\n",
      "        |==>  acc: 0.8774,  nmi: 0.8685  <==|\n",
      "        |==>  acc: 0.8779,  nmi: 0.8694  <==|\n",
      "        |==>  acc: 0.8775,  nmi: 0.8683  <==|\n",
      "        |==>  acc: 0.8778,  nmi: 0.8700  <==|\n",
      "        |==>  acc: 0.8783,  nmi: 0.8701  <==|\n",
      "        |==>  acc: 0.8783,  nmi: 0.8691  <==|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        |==>  acc: 0.8778,  nmi: 0.8690  <==|\n",
      "        |==>  acc: 0.8769,  nmi: 0.8657  <==|\n",
      "        |==>  acc: 0.8772,  nmi: 0.8662  <==|\n",
      "        |==>  acc: 0.8772,  nmi: 0.8666  <==|\n",
      "        |==>  acc: 0.8786,  nmi: 0.8692  <==|\n",
      "        |==>  acc: 0.8775,  nmi: 0.8686  <==|\n",
      "        |==>  acc: 0.8776,  nmi: 0.8680  <==|\n",
      "        |==>  acc: 0.8782,  nmi: 0.8694  <==|\n",
      "        |==>  acc: 0.8781,  nmi: 0.8681  <==|\n",
      "        |==>  acc: 0.8779,  nmi: 0.8694  <==|\n",
      "        |==>  acc: 0.8775,  nmi: 0.8691  <==|\n",
      "        |==>  acc: 0.8770,  nmi: 0.8685  <==|\n",
      "        |==>  acc: 0.8761,  nmi: 0.8666  <==|\n",
      "        |==>  acc: 0.8762,  nmi: 0.8669  <==|\n",
      "        |==>  acc: 0.8768,  nmi: 0.8665  <==|\n",
      "        |==>  acc: 0.8773,  nmi: 0.8668  <==|\n",
      "        |==>  acc: 0.8760,  nmi: 0.8676  <==|\n",
      "        |==>  acc: 0.8775,  nmi: 0.8687  <==|\n",
      "        |==>  acc: 0.8758,  nmi: 0.8655  <==|\n",
      "        |==>  acc: 0.8769,  nmi: 0.8681  <==|\n",
      "        |==>  acc: 0.8757,  nmi: 0.8665  <==|\n",
      "        |==>  acc: 0.8777,  nmi: 0.8688  <==|\n",
      "        |==>  acc: 0.8768,  nmi: 0.8688  <==|\n"
     ]
    }
   ],
   "source": [
    "#now start training\n",
    "import random\n",
    "random.seed(7)\n",
    "dec = DEC(10)\n",
    "#dec.pretrain(train_loader, test_loader, 200)\n",
    "dec.train(train_loader, test_loader, 200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
