{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTC_trainer:\n",
    "    \"\"\"The class for controlling the training process of DEC\"\"\"\n",
    "    \n",
    "    \n",
    "    def target_distribution(self,q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return Variable(((weight.t() / weight.sum(1)).t()).data,requires_grad=False)\n",
    "    \n",
    "    def pretrain(self,train_loader, test_loader, epochs):\n",
    "        \n",
    "        dtc = DTC().cuda() #auto encoder\n",
    "        mseloss = nn.MSELoss()\n",
    "        optimizer = optim.Adam(dtc.parameters())\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            dtc.train()\n",
    "            running_loss=0.0\n",
    "            for i,data in enumerate(train_loader):\n",
    "                x, label = data\n",
    "                x,label=Variable(x).cuda(),Variable(label).cuda()\n",
    "                optimizer.zero_grad()\n",
    "                x_1,x_2 = dtc(x)\n",
    "                td = self.target_distribution(x_1)\n",
    "                loss =  F.kl_div(x_2,td,reduce=True) #mseloss(x_de,x) # so the aim is to minimize the reconstruct error\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # print statistics\n",
    "                running_loss += loss.data.cpu().numpy()[0]\n",
    "                if i % 100 == 99:    # print every 2000 mini-batches\n",
    "                    print('[%d, %5d] loss: %.7f' %\n",
    "                          (epoch + 1, i + 1, running_loss / 100))\n",
    "                    #print('x_de:',x_de, x)\n",
    "                    running_loss = 0.0\n",
    "            #now we evaluate the accuracy with AE\n",
    "            dtc.eval()\n",
    "            for i,data in enumerate(test_loader):\n",
    "                x, label = data\n",
    "                x=Variable(x).cuda()\n",
    "                x_ae,_ = dtc(x)\n",
    "                x_ae = x_ae.data.cpu().numpy()\n",
    "                label = label.cpu().numpy()\n",
    "                y_pred = np.argmax(x_ae,axis=1)\n",
    "                print('y_pred',y_pred)\n",
    "                print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
    "                          % (acc(label, y_pred), nmi(label, y_pred)))\n",
    "                break\n",
    "import random\n",
    "random.seed(7)\n",
    "dtc = DTC_trainer()\n",
    "dtc.pretrain(train_loader, test_loader, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "nmi = normalized_mutual_info_score\n",
    "ari = adjusted_rand_score\n",
    "\n",
    "\n",
    "def acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicUnit(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicUnit,self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.conv_a1 = nn.Conv2d(1,50,4,stride=2,padding=2)\n",
    "        self.conv_a2 = nn.Conv2d(50,50,5,stride=2,padding=2)\n",
    "        self.leReLU = nn.LeakyReLU()\n",
    "        self.fca1 = nn.Linear(50*9*9,68)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform(m.weight)\n",
    "\n",
    "                \n",
    "    def forward(self,x):\n",
    "        # 32x32x1\n",
    "        x = self.dropout(x)        \n",
    "        # 32x32x1\n",
    "        x = self.conv_a1(x)\n",
    "        # 17x17x50\n",
    "        x = self.leReLU(x)\n",
    "        # 17x17x50\n",
    "        x = self.dropout(x)\n",
    "        # 17x17x50\n",
    "        x = self.conv_a2(x)\n",
    "        # 9x9x50\n",
    "        x = self.leReLU(x)\n",
    "        # 9x9x50\n",
    "        x = self.dropout(x)\n",
    "        # 9x9x50\n",
    "        x = x.view(-1, 50*9*9)\n",
    "        # 1x4050\n",
    "        x = self.fca1(x)\n",
    "        # 1x68\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "class DTC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DTC,self).__init__()\n",
    "        self.unit_a = BasicUnit()\n",
    "        self.unit_b = BasicUnit()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x1 = self.unit_a(x)\n",
    "        x2 = self.unit_b(x)\n",
    "        \n",
    "        return x1,x2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
